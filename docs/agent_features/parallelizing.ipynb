{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9b9966d8-e8a3-420a-807d-e562cec309db",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Parallelizing\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de764511-8217-425c-8430-51dd944b8a51",
   "metadata": {},
   "source": [
    "Lasagna AI is built on `asyncio`, so parallelization (cough, _concurrency_) is easy.\n",
    "\n",
    "Also, the `AgentRun` type can accurately capture which agents you ran in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c1411d-ba21-4533-9a40-073fc1bcdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This page will use the following imports:\n",
    "\n",
    "from lasagna import Model, EventCallback, AgentRun\n",
    "from lasagna import (\n",
    "    recursive_extract_messages,\n",
    "    override_system_prompt,\n",
    "    noop_callback,\n",
    "    extraction,\n",
    "    parallel_runs,\n",
    ")\n",
    "from lasagna import known_models\n",
    "from lasagna.tui import tui_input_loop\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3df320-4d98-4ec3-ac76-a47bc893d294",
   "metadata": {},
   "source": [
    "We need to set up our \"binder\" (see the [quickstart guide](../quickstart.ipynb) for what this is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf93849-2141-4a4f-8583-8ece249d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Anthropic\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    print('Using Anthropic')\n",
    "    binder = known_models.anthropic_claude_sonnet_4_5_binder\n",
    "\n",
    "elif os.environ.get('OPENAI_API_KEY'):\n",
    "    print('Using OpenAI')\n",
    "    binder = known_models.openai_gpt_5_mini_binder\n",
    "\n",
    "else:\n",
    "    assert False, \"Neither OPENAI_API_KEY nor ANTHROPIC_API_KEY is set! We need at least one to do this demo.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "626ffc7a-7a82-46b4-9f82-f1a7dafa683f",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## API Limits!\n",
    "\n",
    "Be careful not to exceed your API request (or token count) limits!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220021a-ecc6-4ff9-acc2-4a1a5c1c222c",
   "metadata": {},
   "source": [
    "## A Common Example\n",
    "\n",
    "A common example of wanting to run agents in parallel is when you want to extract different independent information. You can do those independent extractions in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e63fac-307d-4821-b097-9fec24082d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameOutput(BaseModel):\n",
    "    thoughts: str = Field(description='your free-form thoughts')\n",
    "    name_is_known: bool = Field(description=\"true if the user has indicated their name; false otherwise\")\n",
    "    name: str = Field(description=\"the user's name (if known); an empty string otherwise\")\n",
    "\n",
    "\n",
    "@binder\n",
    "async def name_extractor(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "    messages = override_system_prompt(messages, \"You extract the user's name from the conversation.\")\n",
    "    message, result = await model.extract(\n",
    "        noop_callback,\n",
    "        messages = messages,\n",
    "        extraction_type = NameOutput,\n",
    "    )\n",
    "    return extraction('name_extractor', [message], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb0ee95-da1e-4b47-8fd4-eb65b0b1e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeOutput(BaseModel):\n",
    "    thoughts: str = Field(description='your free-form thoughts')\n",
    "    age_is_known: bool = Field(description=\"true if the user has indicated their age; false otherwise\")\n",
    "    age: int = Field(description=\"the user's age (if known); zero otherwise\")\n",
    "\n",
    "\n",
    "@binder\n",
    "async def age_extractor(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "    messages = override_system_prompt(messages, \"You extract the user's age from the conversation.\")\n",
    "    message, result = await model.extract(\n",
    "        noop_callback,\n",
    "        messages = messages,\n",
    "        extraction_type = AgeOutput,\n",
    "    )\n",
    "    return extraction('age_extractor', [message], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44513975-4882-4c2d-8a84-57d9554e65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@binder\n",
    "async def root_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    name_coro = name_extractor(event_callback, prev_runs)\n",
    "    age_coro  = age_extractor(event_callback, prev_runs)\n",
    "\n",
    "    name_run, age_run = await asyncio.gather(name_coro, age_coro)\n",
    "\n",
    "    assert name_run['type'] == 'extraction'\n",
    "    assert age_run['type'] == 'extraction'\n",
    "\n",
    "    print(name_run['result'])\n",
    "    print(age_run['result'])\n",
    "\n",
    "    return parallel_runs('root_agent', [name_run, age_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce991c4-90d3-4ddc-88e6-8aba63af54c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mthoughts='The user has just said \"Hi\" which is a simple greeting. They haven\\'t provided their name or any identifying information in this message.' name_is_known=False name=''\n",
      "thoughts='The user just said \"Hi\" which is a simple greeting. They haven\\'t provided any information about their age, so I don\\'t know how old they are.' age_is_known=False age=0\n",
      "\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  I'm Ryan. Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mthoughts='The user has clearly stated their name is Ryan in their greeting \"Hi I\\'m Ryan.\" They then ask who I am, but my task is to extract their name from the conversation.' name_is_known=True name='Ryan'\n",
      "thoughts='The user introduced themselves as Ryan but did not mention their age at all. I need to indicate that the age is not known and set it to zero as specified.' age_is_known=False age=0\n",
      "\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  I was born in 1945. It's now 2025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mthoughts='The user has clearly stated \"I\\'m Ryan\" so their name is Ryan and they have indicated their name to me. They also provided some additional information about being born in 1945 and that it\\'s now 2025, and asked who I am, but the main task is to extract their name.' name_is_known=True name='Ryan'\n",
      "thoughts=\"The user Ryan mentioned he was born in 1945 and that it's now 2025. I can calculate his age by subtracting his birth year from the current year: 2025 - 1945 = 80 years old. The user has clearly indicated his birth year, so I know his age.\" age_is_known=True age=80\n",
      "\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(root_agent)   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "584048fa-cb09-4dca-95ab-7bd638b64ae0",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Why Different Invocations?\n",
    "\n",
    "Like humans, an AI model often does better when it focuses on a _single_ task. As such, invoking it twice, once for each independent task, often yields better results (as opposed to prompting once and asking for two tasks in your prompt).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bff106-e3c3-45fc-97ee-1b5f2d4c9e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
