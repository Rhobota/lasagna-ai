{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d6734406-dc8b-4727-be3e-9e2055385035",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Lasagna Agent\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81da2c4-2109-4a56-b157-bd53383903b5",
   "metadata": {},
   "source": [
    "In Lasagna AI, an **agent** is a unit of AI-powered reasoning that performs _specific_ work. Agents are the building blocks of your AI system — you compose simple agents together to create powerful multi-agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mqyo5n16yb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This page will use the following imports:\n",
    "\n",
    "from lasagna import Model, EventCallback, AgentRun\n",
    "from lasagna import (\n",
    "    recursive_extract_messages,\n",
    "    extract_last_message,\n",
    "    override_system_prompt,\n",
    "    flat_messages,\n",
    "    parallel_runs,\n",
    "    chained_runs,\n",
    "    extraction,\n",
    ")\n",
    "from lasagna import known_models\n",
    "from lasagna.tui import tui_input_loop\n",
    "\n",
    "import os\n",
    "import re\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67107312-d0e1-4352-b514-5a142a59237e",
   "metadata": {},
   "source": [
    "## What is an _Agent_?\n",
    "\n",
    "A piece of software is an \"agent\" if it displays some sort of \"agency.\" Circular, yes, so let's keep going...\n",
    "\n",
    "### What is _Agency_?\n",
    "\n",
    "_Agency_ is the ability to act on one's own behalf. That is, software has _agency_ if it is allowed to _decide_ and _act_ on its own.\n",
    "\n",
    "- Software that computes π? Not much agency. It does, and always will do, one thing.\n",
    "- Software that organizes your calendar without your input? Lots of agency!\n",
    "\n",
    "### What is an _AI Agent_?\n",
    "\n",
    "The phrase \"AI Agent\" has risen in popularity since ~2024. Typically, when people use this phrase, they mean a piece of software that uses a Large Language Model (LLM\\*) and has some _tool calling_ capability so that it can _affect_ the world (e.g. send emails, query for today's news, organize your calendar, etc.)\n",
    "\n",
    "This is consistent with the idea of _agency_ from above. An LLM, on its own, has no _agency_ (it just spits tokens at you). If you connect that _same_ LLM to software functions (via _tool calling_), then suddenly the LLM gains the ability to _act_, and people start calling it \"agentic\"."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e87b1fc4-0222-4f3e-bbf9-7c8bdd767357",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## \\*LLM vs `Model`\n",
    "\n",
    "Lasagna AI tries not to use the term \"LLM,\" but instead uses the term `Model` to refer to generative AI models. This is merely an attempt to be more generic and avoid questions like \"how large is large?\" and \"what about multimodal models?\"\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcfc1a-68f8-4b3e-b2cb-f4cf490efff8",
   "metadata": {},
   "source": [
    "### How does _Lasagna AI_ define an Agent?\n",
    "\n",
    "Everything above ☝️ is too theoretical. When the rubber meets the road, what actually _is_ an **agent** inside _Lasagna AI_?\n",
    "\n",
    "In _Lasagna_, an **agent** is a unit of work that leverages a `Model`.\n",
    "\n",
    "Think of an agent as a specialized _worker_ that:\n",
    "\n",
    "1. **Analyzes** the current situation.\n",
    "2. **Decides** what needs to be done.\n",
    "3. **Acts** using AI models and tools.\n",
    "4. **Records** its output.\n",
    "\n",
    "In Lasagna, agents are _composable_. You begin by developing individual agents, each with a narrow focus, then you _compose_ them together into a complex multi-agent system. Like humans, it's helpful to decompose and delegate tasks amongst the group.\n",
    "\n",
    "In the next section, we'll see how to write **agents** using Lasagna AI's interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ont02120yu",
   "metadata": {},
   "source": [
    "## The Agent Interface\n",
    "\n",
    "Every Lasagna agent follows the same pattern — it's a _callable_ (either a function or a callable-object) that takes exactly **three parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tds0cpw956o",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def my_agent(\n",
    "    model: Model,                   # ← the AI model available to this agent\n",
    "    event_callback: EventCallback,  # ← used for streaming and event handling\n",
    "    prev_runs: list[AgentRun],      # ← previous work, context, or conversation history\n",
    ") -> AgentRun:\n",
    "    # Agent logic goes here...\n",
    "    raise RuntimeError('not yet implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7wrp2g8an",
   "metadata": {},
   "source": [
    "Let's understand what each parameter represents:\n",
    "\n",
    "- **`model`**: The AI model (like GPT-4o, Claude, etc.) that your agent can use for reasoning, text generation, or decision-making.\n",
    "- **`event_callback`**: A function for handling streaming events and progress updates. This enables real-time feedback as your agent works.\n",
    "- **`prev_runs`**: The history of previous work. In a conversation, this contains past messages. In a workflow, this contains results from earlier steps.\n",
    "\n",
    "The agent returns an `AgentRun` — a structured representation of what the agent generated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1651ee86-c49e-40d3-8f59-ce97983f0389",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## 📌 Put a pin in `AgentRun`\n",
    "\n",
    "For now, just know that `AgentRun` is a core data type. We'll explore `AgentRun` in detail in the [next chapter](type_agentrun.ipynb)!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q37aac6nazp",
   "metadata": {},
   "source": [
    "## How do I write an agent?\n",
    "\n",
    "When you sit down to write an agent, here is what you must consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fc110-1597-481a-a28a-174d5ae6ae56",
   "metadata": {},
   "source": [
    "### 1. Analyze the Current Situation\n",
    "\n",
    "Your agent must _examine_ `prev_runs` to understand what has happened so far. It may find:\n",
    "\n",
    "- previous messages in a conversation,\n",
    "- results from earlier agents in a workflow, and/or\n",
    "- intermediate outputs from a multi-step process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eec636d-9b58-4fed-a32b-cf3e5211c3bb",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Analysis\" is Trivial\n",
    "\n",
    "It's common that an agent will _expect_ certain types of messages, so no real \"examination\" takes place. In those cases, the agent may simply `assert` what it expects.\n",
    "\n",
    "Otherwise, the agent is free to filter/clean/reformulate/branch-off `prev_runs` in any way you see fit!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34d9d9-55cf-4ce8-bfe5-1224cc74c5ce",
   "metadata": {},
   "source": [
    "### 2. Make Behavioral Decisions\n",
    "\n",
    "Your agent must _decide_ what to do next. It might:\n",
    "\n",
    "- generate a response using the AI `model`, or\n",
    "- use the AI `model` to extract data, or\n",
    "- pass tools to the AI `model`, or\n",
    "- split its task into multiple subtasks and delegate those to downstream agents, or\n",
    "- do _many_ of the things above as many times as it chooses!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22306bd5-daa5-4b62-adbd-7b804c8f6d91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Decision\" is Trivial\n",
    "\n",
    "It's common that an agent does _not_ \"decide\" anything on-the-fly; rather, you may write your agent to _always do_ one of the things above. However, you are free to write an agent that _does_ decide on-the-fly (perhaps via help from the AI `model` or a downstream agent), as you see fit.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0828a0-77e7-4116-a558-2fa972c61a5e",
   "metadata": {},
   "source": [
    "### 3. Take Action\n",
    "\n",
    "Your agent must _execute_ its decision:\n",
    "\n",
    "- **Model interaction**: Invoke the AI `model` to generate text, reason about problems, or extract structured outputs.\n",
    "- **Tool usage**: Send emails, query a database, etc.\n",
    "- **Agent delegation**: Invoke downstream agents to handle subtasks."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f718a423-82d0-4d4b-bd37-27e000bedf91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Good ol' Programming Time\n",
    "\n",
    "This is good-ol' hands-on-the-keyboard write-Python-code. This is you writing code to (1) invoke the AI `model` with the correct inputs, (2) grab the AI response and make use of it, (3) connect your agents together, and (4) connect your agents to the rest of your software stack. Get to it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ef052-5b4d-4777-a3ad-6b38869c0a5d",
   "metadata": {},
   "source": [
    "### 4. Record its Output\n",
    "\n",
    "Your agent must _construct_ and _return_ an `AgentRun` that contains:\n",
    "\n",
    "- any new information that it generated, and/or\n",
    "- results from sub-agents it coordinated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf0e6ca0-a3e3-4e6b-bb83-b603aabdd269",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## This is a critical step!\n",
    "\n",
    "The `AgentRun` you return here will be passed as input to other agents (or to your same agent, in the case of multi-turn chat). It's _critical_ that you record everything that happened and return it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmj16o92rw",
   "metadata": {},
   "source": [
    "## Real Agent Examples\n",
    "\n",
    "Let's look at some **real** examples to see agents in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0084a-1eee-493c-aa77-3b29c2be814a",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we write and run some agents, we need to set up our \"binder\" (see the [quickstart guide](../quickstart.ipynb) for what this is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24107132-fa62-4eef-b641-359a4e0b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Anthropic\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    print('Using Anthropic')\n",
    "    binder = known_models.anthropic_claude_sonnet_4_binder\n",
    "\n",
    "elif os.environ.get('OPENAI_API_KEY'):\n",
    "    print('Using OpenAI')\n",
    "    binder = known_models.openai_gpt_5_mini_binder\n",
    "\n",
    "else:\n",
    "    assert False, \"Neither OPENAI_API_KEY nor ANTHROPIC_API_KEY is set! We need at least one to do this demo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noblz7bden",
   "metadata": {},
   "source": [
    "### The Conversational Agent\n",
    "\n",
    "This is the simplest type of agent — it uses the message history to generate a new text response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a76b04-26e3-4296-8f50-e0391af3a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Use the model to generate a _new_ response:\n",
    "    new_messages = await model.run(event_callback, messages, tools=[])\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('chat_agent', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tnlh8nkl1o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi, I'm Ryan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0mHi Ryan! Nice\u001b[0m\u001b[0m to meet you. How are you doing today?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0mYour\u001b[0m\u001b[0m name is Ryan - you introduce\u001b[0m\u001b[0md yourself to me at the start of our conversation.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(chat_agent))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6zm8snjnbwu",
   "metadata": {},
   "source": [
    "### The Specialist Agent\n",
    "\n",
    "Agents can be specialized for particular tasks. Here's an agent that focuses on providing helpful coding advice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb6728e-f852-4f5e-b533-1873a052a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODING_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful coding assistant named Bob.\n",
    "Provide clear, practical advice with code examples when appropriate.\n",
    "Focus on best practices and explain your reasoning.\n",
    "Answer all prompts in one sentence!\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac767793-50bf-4ecb-9267-233a51e0e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def coding_advisor(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Generate a response with an OVERRIDDEN system prompt!\n",
    "    modified_messages = override_system_prompt(messages, CODING_SYSTEM_PROMPT)\n",
    "    new_messages = await model.run(event_callback, modified_messages, tools=[])\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('coding_advisor', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "offgo6balkj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm Bob\u001b[0m\u001b[0m, a helpful coding assistant designed to provide clear, practical programming\u001b[0m\u001b[0m advice with code examples and best practices explanations.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How do I add numbers in Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0mYou\u001b[0m\u001b[0m can add numbers in Python using the\u001b[0m\u001b[0m `+` operator like\u001b[0m\u001b[0m `result = 5 + 3`\u001b[0m\u001b[0m for integers/floats,\u001b[0m\u001b[0m or use the `sum()` function for lists\u001b[0m\u001b[0m like `total = sum([1, 2,\u001b[0m\u001b[0m 3,\u001b[0m\u001b[0m 4])` which is\u001b[0m\u001b[0m best practice for adding\u001b[0m\u001b[0m multiple numbers efficiently\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(coding_advisor))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24607c-e56e-485d-a0c5-bed4ff758696",
   "metadata": {},
   "source": [
    "### The Information Extractor\n",
    "\n",
    "Let's make an agent that does **structured output** to _extract_ information from the user's message. In particular, we'll have this agent classify the user's message (i.e. it is \"extracting\" the classification, if you will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf224901-b74f-49a5-95e2-074c14b6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_CLASSIFIER_SYSTEM_PROMPT = \"\"\"\n",
    "Your job is to classify the user's message into one of the following categories:\n",
    " - `small_talk`: Comments like \"hi\", \"how are you?\", etc.\n",
    " - `programming`: Questions or comments about programming languages, libraries, etc.\n",
    " - `other`: Any message that is not small talk and not programming.\n",
    "\"\"\".strip()\n",
    "\n",
    "# In a production-grade system, you'd probably expand your system prompt to\n",
    "# be more thorough; we're going for minimal here to keep this demo short.\n",
    "\n",
    "class Category(Enum):\n",
    "    small_talk = 'small_talk'\n",
    "    programming = 'programming'\n",
    "    other = 'other'\n",
    "\n",
    "class CategoryOutput(BaseModel):\n",
    "    thoughts: str\n",
    "    category: Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94b9bee6-9795-43c4-8375-5b00ae442c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def intent_classifier(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Get **ONLY** the last message from the conversation so far:\n",
    "    #    (Just for demo-purposes, to show you can do whatever you want with\n",
    "    #     `prev_runs` 😁. A production-grade intent classifier would consider\n",
    "    #     more than just the last message.)\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Generate a structured output response with an OVERRIDDEN system prompt!\n",
    "    messages = override_system_prompt([last_message], INTENT_CLASSIFIER_SYSTEM_PROMPT)\n",
    "    new_message, result = await model.extract(event_callback, messages, CategoryOutput)\n",
    "    assert isinstance(result, CategoryOutput)\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return extraction('intent_classifier', [new_message], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8843b12-5cb7-4ed3-acef-cbf16ae087f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"thought\u001b[0m\u001b[31ms\": \u001b[0m\u001b[31m\"This i\u001b[0m\u001b[31ms a simple\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m \\\"Hi!\\\u001b[0m\u001b[31m\" which\u001b[0m\u001b[31m is a classi\u001b[0m\u001b[31mc exampl\u001b[0m\u001b[31me of s\u001b[0m\u001b[31mmall\u001b[0m\u001b[31m talk -\u001b[0m\u001b[31m a cas\u001b[0m\u001b[31mual, friend\u001b[0m\u001b[31mly gre\u001b[0m\u001b[31meting \u001b[0m\u001b[31mus\u001b[0m\u001b[31med to \u001b[0m\u001b[31minit\u001b[0m\u001b[31miate c\u001b[0m\u001b[31monversati\u001b[0m\u001b[31mon.\"\u001b[0m\u001b[31m, \"category\"\u001b[0m\u001b[31m: \"smal\u001b[0m\u001b[31ml_ta\u001b[0m\u001b[31mlk\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Sup?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"thou\u001b[0m\u001b[31mghts\"\u001b[0m\u001b[31m: \"T\u001b[0m\u001b[31mhis is \u001b[0m\u001b[31ma casual gre\u001b[0m\u001b[31meti\u001b[0m\u001b[31mng s\u001b[0m\u001b[31mimil\u001b[0m\u001b[31mar to \\\"\u001b[0m\u001b[31mwhat's up\\\"\u001b[0m\u001b[31m or \\\"hi\u001b[0m\u001b[31m\\\"\u001b[0m\u001b[31m. This f\u001b[0m\u001b[31malls u\u001b[0m\u001b[31mnder small\u001b[0m\u001b[31m talk a\u001b[0m\u001b[31ms it's an\u001b[0m\u001b[31m infor\u001b[0m\u001b[31mmal way to\u001b[0m\u001b[31m s\u001b[0m\u001b[31may hello.\u001b[0m\u001b[31m\"\u001b[0m\u001b[31m, \u001b[0m\u001b[31m\"category\":\u001b[0m\u001b[31m \"smal\u001b[0m\u001b[31ml_talk\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"thoug\u001b[0m\u001b[31mhts\": \"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user i\u001b[0m\u001b[31ms asking \\\u001b[0m\u001b[31m\"What is \u001b[0m\u001b[31mPython?\\\" \u001b[0m\u001b[31mwhic\u001b[0m\u001b[31mh is clearly\u001b[0m\u001b[31m a ques\u001b[0m\u001b[31mtion abo\u001b[0m\u001b[31mut a pro\u001b[0m\u001b[31mgramm\u001b[0m\u001b[31ming\u001b[0m\u001b[31m langua\u001b[0m\u001b[31mge. Pytho\u001b[0m\u001b[31mn is a popu\u001b[0m\u001b[31mlar pro\u001b[0m\u001b[31mgr\u001b[0m\u001b[31mamming langu\u001b[0m\u001b[31mage, s\u001b[0m\u001b[31mo t\u001b[0m\u001b[31mhis fall\u001b[0m\u001b[31ms u\u001b[0m\u001b[31mnder the\u001b[0m\u001b[31m programmi\u001b[0m\u001b[31mng cate\u001b[0m\u001b[31mgory\u001b[0m\u001b[31m.\"\u001b[0m\u001b[31m, \"cat\u001b[0m\u001b[31megory\"\u001b[0m\u001b[31m: \"pr\u001b[0m\u001b[31mogram\u001b[0m\u001b[31mming\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthough\u001b[0m\u001b[31mts\": \"\u001b[0m\u001b[31mTh\u001b[0m\u001b[31me user is \u001b[0m\u001b[31maskin\u001b[0m\u001b[31mg a basic m\u001b[0m\u001b[31math question\u001b[0m\u001b[31m \\\u001b[0m\u001b[31m\"What i\u001b[0m\u001b[31ms 2+2?\\\". T\u001b[0m\u001b[31mhis is a \u001b[0m\u001b[31msimple a\u001b[0m\u001b[31mrith\u001b[0m\u001b[31mme\u001b[0m\u001b[31mtic qu\u001b[0m\u001b[31mestion\u001b[0m\u001b[31m that doe\u001b[0m\u001b[31msn't fall \u001b[0m\u001b[31munder s\u001b[0m\u001b[31mmall t\u001b[0m\u001b[31malk (lik\u001b[0m\u001b[31me greetings)\u001b[0m\u001b[31m or \u001b[0m\u001b[31mprog\u001b[0m\u001b[31mrammi\u001b[0m\u001b[31mng (que\u001b[0m\u001b[31mstions about\u001b[0m\u001b[31m code, la\u001b[0m\u001b[31mng\u001b[0m\u001b[31muages,\u001b[0m\u001b[31m libra\u001b[0m\u001b[31mries).\u001b[0m\u001b[31m This i\u001b[0m\u001b[31ms a ma\u001b[0m\u001b[31mthematical q\u001b[0m\u001b[31muestion, \u001b[0m\u001b[31mso it belo\u001b[0m\u001b[31mng\u001b[0m\u001b[31ms in t\u001b[0m\u001b[31mhe \\\"o\u001b[0m\u001b[31mther\\\" c\u001b[0m\u001b[31mateg\u001b[0m\u001b[31mor\u001b[0m\u001b[31my.\"\u001b[0m\u001b[31m, \u001b[0m\u001b[31m\"c\u001b[0m\u001b[31mategory\u001b[0m\u001b[31m\": \"oth\u001b[0m\u001b[31mer\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(intent_classifier))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adebd9d-959c-4d6d-8863-04e8f9739c4c",
   "metadata": {},
   "source": [
    "### The 'Back on Track' Agent\n",
    "\n",
    "This agent is pretty useless on its own, but you'll see soon why we're creating it. It just tells the user to get back on track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5caf5a42-f514-4b34-80e3-55caa4dac78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_ON_TRACK_SYSTEM_PROMPT = \"\"\"\n",
    "The user's message has been deemed to be off-topic.\n",
    "Please politely tell them that their message is off-topic.\n",
    "Do not respond to their question or their request. Just politely\n",
    "tell them they are off-topic and need to return to the topic\n",
    "at-hand.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "430071f2-60b4-4d0d-a3d1-6e952b46633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def back_on_track(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    messages = override_system_prompt(\n",
    "        [last_message],\n",
    "        BACK_ON_TRACK_SYSTEM_PROMPT,\n",
    "    )\n",
    "    return flat_messages(\n",
    "        'back_on_track',\n",
    "        await model.run(event_callback, messages, tools=[]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cfa7ba1-ac85-45a1-9952-4d4c96c5183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m! I\u001b[0m\u001b[0m'\u001b[0m\u001b[0md be happy to help you,\u001b[0m\u001b[0m but it seems like your\u001b[0m\u001b[0m message might be off-topic for\u001b[0m\u001b[0m our current discussion. Coul\u001b[0m\u001b[0md you please share what\u001b[0m\u001b[0m specific topic or question you'd like to discuss\u001b[0m\u001b[0m so we can have a focuse\u001b[0m\u001b[0md conversation? I'm here to assist\u001b[0m\u001b[0m once we establish\u001b[0m\u001b[0m what you'd like to talk about.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(back_on_track))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a120-96be-4b0a-a593-562304c314c0",
   "metadata": {},
   "source": [
    "### The Routing Agent\n",
    "\n",
    "Now, we'll put all the pieces together by making a **routing agent** that uses **all four** agents above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb0e673-a73c-477c-8e5b-07f842c34baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def router(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Decide which downstream agent to use, based on the user's intent:\n",
    "    bound_intent_classifier = binder(intent_classifier)\n",
    "    classification_run = await bound_intent_classifier(event_callback, prev_runs)\n",
    "    assert classification_run['type'] == 'extraction'\n",
    "    classification_result = classification_run['result']\n",
    "    assert isinstance(classification_result, CategoryOutput)\n",
    "    if classification_result.category == Category.small_talk:\n",
    "        downstream_agent = binder(chat_agent)\n",
    "    elif classification_result.category == Category.programming:\n",
    "        downstream_agent = binder(coding_advisor)\n",
    "    else:\n",
    "        downstream_agent = binder(back_on_track)\n",
    "\n",
    "    # Delegate to the downstream agent!\n",
    "    downstream_run = await downstream_agent(event_callback, prev_runs)\n",
    "\n",
    "    # Wrap *everything* that happened above into the return:\n",
    "    return chained_runs('router', [classification_run, downstream_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd2ba67-1c00-4ce5-be97-b62bdc4db220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mth\u001b[0m\u001b[31mough\u001b[0m\u001b[31mts\":\u001b[0m\u001b[31m \"The us\u001b[0m\u001b[31mer\u001b[0m\u001b[31m simpl\u001b[0m\u001b[31my said \\\"Hi!\u001b[0m\u001b[31m\\\" which\u001b[0m\u001b[31m is a bas\u001b[0m\u001b[31mic greetin\u001b[0m\u001b[31mg \u001b[0m\u001b[31mand falls \u001b[0m\u001b[31munder\u001b[0m\u001b[31m casual \u001b[0m\u001b[31mconversati\u001b[0m\u001b[31mon or \u001b[0m\u001b[31msmall tal\u001b[0m\u001b[31mk.\"\u001b[0m\u001b[31m, \"category\"\u001b[0m\u001b[31m: \"small_\u001b[0m\u001b[31mta\u001b[0m\u001b[31mlk\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello! How are\u001b[0m\u001b[0m you doing today? Is\u001b[0m\u001b[0m there anything I can help you with?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python? (answer briefly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"th\u001b[0m\u001b[31mought\u001b[0m\u001b[31ms\": \u001b[0m\u001b[31m\"The user \u001b[0m\u001b[31mis \u001b[0m\u001b[31masking \\\"Wh\u001b[0m\u001b[31mat is\u001b[0m\u001b[31m Python?\u001b[0m\u001b[31m\\\" which\u001b[0m\u001b[31m is clearl\u001b[0m\u001b[31my a question\u001b[0m\u001b[31m about a pro\u001b[0m\u001b[31mgramming \u001b[0m\u001b[31mlanguage. Th\u001b[0m\u001b[31mis falls \u001b[0m\u001b[31munder the pr\u001b[0m\u001b[31mogramming ca\u001b[0m\u001b[31mtegor\u001b[0m\u001b[31my as\u001b[0m\u001b[31m they'r\u001b[0m\u001b[31me aski\u001b[0m\u001b[31mng abou\u001b[0m\u001b[31mt Pyth\u001b[0m\u001b[31mon, \u001b[0m\u001b[31mwhich is a w\u001b[0m\u001b[31mell-kn\u001b[0m\u001b[31mown program\u001b[0m\u001b[31mming lang\u001b[0m\u001b[31muage.\"\u001b[0m\u001b[31m, \"ca\u001b[0m\u001b[31mtegory\": \"p\u001b[0m\u001b[31mrogramming\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mPython is a high\u001b[0m\u001b[0m-level, interpreted programming language known for its simple\u001b[0m\u001b[0m syntax and versat\u001b[0m\u001b[0mility, making it popular for web\u001b[0m\u001b[0m development, data science, automation\u001b[0m\u001b[0m, and many other applications.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"thou\u001b[0m\u001b[31mghts\": \"Thi\u001b[0m\u001b[31ms is a \u001b[0m\u001b[31mba\u001b[0m\u001b[31msic \u001b[0m\u001b[31marithmetic\u001b[0m\u001b[31m q\u001b[0m\u001b[31muestion \u001b[0m\u001b[31mas\u001b[0m\u001b[31mking \u001b[0m\u001b[31mfor t\u001b[0m\u001b[31mhe sum of 2\u001b[0m\u001b[31m+2. It's \u001b[0m\u001b[31mnot\u001b[0m\u001b[31m small tal\u001b[0m\u001b[31mk (l\u001b[0m\u001b[31mike greetin\u001b[0m\u001b[31mgs or cas\u001b[0m\u001b[31mual \u001b[0m\u001b[31mconversati\u001b[0m\u001b[31mon), \u001b[0m\u001b[31mand it's not\u001b[0m\u001b[31m a\u001b[0m\u001b[31mbout pr\u001b[0m\u001b[31mogramm\u001b[0m\u001b[31ming (no me\u001b[0m\u001b[31mnti\u001b[0m\u001b[31mon of code\u001b[0m\u001b[31m, la\u001b[0m\u001b[31mnguages, l\u001b[0m\u001b[31mibraries, \u001b[0m\u001b[31metc.). This\u001b[0m\u001b[31m falls i\u001b[0m\u001b[31mnt\u001b[0m\u001b[31mo the \\\"othe\u001b[0m\u001b[31mr\\\" categor\u001b[0m\u001b[31my as \u001b[0m\u001b[31mit's a \u001b[0m\u001b[31mmat\u001b[0m\u001b[31mhematical\u001b[0m\u001b[31m question.\u001b[0m\u001b[31m\"\u001b[0m\u001b[31m, \u001b[0m\u001b[31m\"cate\u001b[0m\u001b[31mgory\": \u001b[0m\u001b[31m\"othe\u001b[0m\u001b[31mr\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI appreciate\u001b[0m\u001b[0m your question, but it appears\u001b[0m\u001b[0m to be off-topic for our current\u001b[0m\u001b[0m conversation. We should focus on staying\u001b[0m\u001b[0m on the subject we were discussing.\u001b[0m\u001b[0m Please feel free to redirect\u001b[0m\u001b[0m your question or comment back\u001b[0m\u001b[0m to the main topic at hand.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How are you today? (also what is 2+2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"thoug\u001b[0m\u001b[31mhts\"\u001b[0m\u001b[31m: \"This me\u001b[0m\u001b[31mssag\u001b[0m\u001b[31me contains\u001b[0m\u001b[31m both smal\u001b[0m\u001b[31ml talk \u001b[0m\u001b[31m(\\\"Ho\u001b[0m\u001b[31mw are\u001b[0m\u001b[31m you today\u001b[0m\u001b[31m?\\\") and a \u001b[0m\u001b[31mbasic mat\u001b[0m\u001b[31mh \u001b[0m\u001b[31mquesti\u001b[0m\u001b[31mon\u001b[0m\u001b[31m (\\\"wh\u001b[0m\u001b[31mat is 2+\u001b[0m\u001b[31m2\\\"). W\u001b[0m\u001b[31mhil\u001b[0m\u001b[31me the mat\u001b[0m\u001b[31mh questio\u001b[0m\u001b[31mn \u001b[0m\u001b[31mcould be co\u001b[0m\u001b[31mnside\u001b[0m\u001b[31mred e\u001b[0m\u001b[31mducation\u001b[0m\u001b[31mal, it's \u001b[0m\u001b[31mvery simple \u001b[0m\u001b[31marith\u001b[0m\u001b[31mmetic ra\u001b[0m\u001b[31mther t\u001b[0m\u001b[31mhan p\u001b[0m\u001b[31mrogrammin\u001b[0m\u001b[31mg-spe\u001b[0m\u001b[31mcif\u001b[0m\u001b[31mic content\u001b[0m\u001b[31m. The \u001b[0m\u001b[31mprimary gre\u001b[0m\u001b[31meting \u001b[0m\u001b[31mnatu\u001b[0m\u001b[31mre of \u001b[0m\u001b[31mthe me\u001b[0m\u001b[31mssag\u001b[0m\u001b[31me and the c\u001b[0m\u001b[31masual \u001b[0m\u001b[31mwa\u001b[0m\u001b[31my both part\u001b[0m\u001b[31ms are \u001b[0m\u001b[31mpresented su\u001b[0m\u001b[31mgg\u001b[0m\u001b[31mes\u001b[0m\u001b[31mts this\u001b[0m\u001b[31m is pri\u001b[0m\u001b[31mma\u001b[0m\u001b[31mril\u001b[0m\u001b[31my small t\u001b[0m\u001b[31malk with\u001b[0m\u001b[31m a casu\u001b[0m\u001b[31mal ques\u001b[0m\u001b[31mtion added\u001b[0m\u001b[31m.\"\u001b[0m\u001b[31m, \"categ\u001b[0m\u001b[31mor\u001b[0m\u001b[31my\": \"\u001b[0m\u001b[31msmall_t\u001b[0m\u001b[31malk\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm doing well, thank you for\u001b[0m\u001b[0m asking! I'm here and ready to\u001b[0m\u001b[0m help with whatever you need.\n",
      "\n",
      "An\u001b[0m\u001b[0md 2+2 =\u001b[0m\u001b[0m 4.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(router))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2b8d2f0-5c19-4bcf-ad52-02777e17478d",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## 🚨 Hacked!\n",
    "\n",
    "Did you notice above how we prompt-hacked our simple AI system? **YIKES!** (Look closely, we were able to get it to answer our `2 + 2` question on the second attempt above, by \"hiding\" the question in a small-talk message.)\n",
    "\n",
    "This is a good time to call out that AI models can (and will!) make mistakes. Prompt engineering helps, but even the most well-tuned prompting cannot protect your AI system from malicious users.\n",
    "\n",
    "Design your AI systems accordingly, and consult best-practice literature. The most important thing: Design your AI system so that it does no damage even _if_ (or _when_) it misbehaves.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115869f-274f-4eac-b32c-e3bf5cbacf8a",
   "metadata": {},
   "source": [
    "### The Task Splitter\n",
    "\n",
    "Another common task is for an agent to split work and delegate to multiple downstream agents. Let's do that next!\n",
    "\n",
    "We'll use a silly example, for simplicity and brevity, where we'll split the user's message into individual sentences, then prompt an AI `model` one-at-a-time on each individual sentence. While this is a silly example, it shows how you can split up a problem for multiple downstream subagents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1721848-54a8-4ffd-b9a7-7887b1e13a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def splitter(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # We'll only look at the most recent message:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    assert last_message['role'] == 'human'\n",
    "    assert last_message['text']\n",
    "\n",
    "    # We'll split the most recent message into sentences:\n",
    "    #    (This is **not** a robust way to do it, but we're keeping the demo simple.)\n",
    "    sentences = re.split(r'[\\.\\?\\!] ', last_message['text'])\n",
    "    sentences_as_agentruns = [\n",
    "        flat_messages('splitter', [{\n",
    "            'role': 'human',\n",
    "            'text': sentence,\n",
    "        }])\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    # Have the `chat_agent` respond to each sentence:\n",
    "    #    (Again, not particularly useful, but good for a brief demo.)\n",
    "    bound_chat_agent = binder(chat_agent)\n",
    "    downstream_runs: list[AgentRun] = []\n",
    "    for task_input in sentences_as_agentruns:\n",
    "        this_run = await bound_chat_agent(event_callback, [task_input])\n",
    "        downstream_runs.append(this_run)\n",
    "\n",
    "    # Wrap *everything* that happened above into the return:\n",
    "    return parallel_runs('splitter', downstream_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974bc078-75fd-4958-ade4-d46b14cf0ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi. What's up? How are you? What's 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello! How are\u001b[0m\u001b[0m you doing today? Is there anything I can help you with?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m! Not much going on here - just ready\u001b[0m\u001b[0m to chat an\u001b[0m\u001b[0md help with whatever you'\u001b[0m\u001b[0md like to talk about or\u001b[0m\u001b[0m work on. How\u001b[0m\u001b[0m are you doing today? What's\u001b[0m\u001b[0m on your mind?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm doing well, thank you for\u001b[0m\u001b[0m asking! I'm here and ready to\u001b[0m\u001b[0m help with whatever you'd like to discuss\u001b[0m\u001b[0m or work on. How\u001b[0m\u001b[0m are you doing today?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m2 \u001b[0m\u001b[0m+ 2 = 4\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Thanks! What did I just say?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYou're welcome! Feel\u001b[0m\u001b[0m free to ask if you need help with anything else.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI\u001b[0m\u001b[0m don't have any recor\u001b[0m\u001b[0md of previous messages from\u001b[0m\u001b[0m you in our conversation. Your\u001b[0m\u001b[0m message \"What did I just say?\" appears to\u001b[0m\u001b[0m be the first thing you\u001b[0m\u001b[0m've said to me in this\u001b[0m\u001b[0m chat session. \n",
      "\n",
      "If you meant\u001b[0m\u001b[0m to reference something you said earlier, could you please repeat\u001b[0m\u001b[0m it or provide more context? I'm\u001b[0m\u001b[0m happy to help once I understand what you're referring\u001b[0m\u001b[0m to.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(splitter))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd9414fb-c54c-41a3-a25a-b24f1d67cc67",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## What happened?\n",
    "\n",
    "In the conversation above, when I said \"What did I just say?\", the AI model didn't _see_ the conversation history. Why not? It's because of how we wrote our agent — we did not pass the whole conversation to the AI model!\n",
    "\n",
    "**Exercise for the reader:** How can you change the agent so that the AI _sees_ the whole conversation history?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca62ce9-3346-4f03-8260-f3d34f7b6cd6",
   "metadata": {},
   "source": [
    "### Do anything!\n",
    "\n",
    "It's up to you how to write your multi-agent AI system. You can mix-and-match ideas, include lots of behaviors in a _single_ agent, or split up tasks among _multiple_ agents. You can have \"meta agents\" that plan work for other agents, or \"meta meta agents\" that plan work for your \"meta agents\". As long as it is _safe_ and _works_, go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8hujw6nu5o",
   "metadata": {},
   "source": [
    "## Why This Design?\n",
    "\n",
    "Lasagna's agent design provides several key benefits:\n",
    "\n",
    "### 🔌 **Pluggability** \n",
    "\n",
    "Every agent follows the same interface, so you can:\n",
    "\n",
    "- swap one agent for another,\n",
    "- combine agents from different sources, and\n",
    "- test agents in isolation.\n",
    "\n",
    "### 🥞 **Layering**\n",
    "\n",
    "You can compose agents at any level:\n",
    "\n",
    "- Use simple agents as building blocks.\n",
    "- Combine them into more complex workflows.\n",
    "- Build entire systems from agent compositions.\n",
    "\n",
    "### 🔄 **Reusability**\n",
    "\n",
    "Write an agent once, use it everywhere:\n",
    "\n",
    "- as a standalone agent,\n",
    "- as part of a larger workflow, or\n",
    "- as a specialist in a multi-agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7yu95a7mrk",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand what agents are and how they work conceptually, you're ready to dive deeper into the technical details.\n",
    "\n",
    "In the [next section](type_agentrun.ipynb), we'll explore the `AgentRun` data structure in detail — the standardized format that enables all this agent composition and layering.\n",
    "\n",
    "You'll learn about:\n",
    "\n",
    "- The four types of `AgentRun`.\n",
    "- How to work with the recursive data structure.\n",
    "- Helper functions for common patterns.\n",
    "- Advanced features like cost tracking and serialization.\n",
    "\n",
    "For more advanced agent patterns and real-world examples, check out:\n",
    "\n",
    "- [Tool Use](../agent_features/tools.ipynb) — Agents that interact with external systems\n",
    "- [Structured Output](../agent_features/structured_output.ipynb) — Agents that extract structured data\n",
    "- [Layering](../agent_features/layering.ipynb) — Complex multi-agent compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ba306-7f00-4953-b96e-143de89597c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
