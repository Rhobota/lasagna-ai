{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d6734406-dc8b-4727-be3e-9e2055385035",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Lasagna Agent\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81da2c4-2109-4a56-b157-bd53383903b5",
   "metadata": {},
   "source": [
    "In Lasagna AI, an **agent** is a unit of AI-powered reasoning that performs _specific_ work. Agents are the building blocks of your AI system â€” you compose simple agents together to create powerful multi-agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mqyo5n16yb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This page will use the following imports:\n",
    "\n",
    "from lasagna import Model, EventCallback, AgentRun\n",
    "from lasagna import (\n",
    "    recursive_extract_messages,\n",
    "    extract_last_message,\n",
    "    override_system_prompt,\n",
    "    flat_messages,\n",
    "    parallel_runs,\n",
    "    chained_runs,\n",
    "    extraction,\n",
    ")\n",
    "from lasagna import known_models\n",
    "from lasagna.tui import tui_input_loop\n",
    "\n",
    "import os\n",
    "import re\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67107312-d0e1-4352-b514-5a142a59237e",
   "metadata": {},
   "source": [
    "## What is an _Agent_?\n",
    "\n",
    "A piece of software is an \"agent\" if it displays some sort of \"agency.\" Circular, yes, so let's keep going...\n",
    "\n",
    "### What is _Agency_?\n",
    "\n",
    "_Agency_ is the ability to act on one's own behalf. That is, software has _agency_ if it is allowed to _decide_ and _act_ on its own.\n",
    "\n",
    "- Software that computes Ï€? Not much agency. It does, and always will do, one thing.\n",
    "- Software that organizes your calendar without your input? Lots of agency!\n",
    "\n",
    "### What is an _AI Agent_?\n",
    "\n",
    "The phrase \"AI Agent\" has risen in popularity since ~2024. Typically, when people use this phrase, they mean a piece of software that uses a Large Language Model (LLM\\*) and has some _tool calling_ capability so that it can _affect_ the world (e.g. send emails, query for today's news, organize your calendar, etc.)\n",
    "\n",
    "This is consistent with the idea of _agency_ from above. An LLM, on its own, has no _agency_ (it just spits tokens at you). If you connect that _same_ LLM to software functions (via _tool calling_), then suddenly the LLM gains the ability to _act_, and people start calling it \"agentic\"."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e87b1fc4-0222-4f3e-bbf9-7c8bdd767357",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## \\*LLM vs `Model`\n",
    "\n",
    "Lasagna AI tries not to use the term \"LLM,\" but instead uses the term `Model` to refer to generative AI models. This is merely an attempt to be more generic and avoid questions like \"how large is large?\" and \"what about multimodal models?\"\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcfc1a-68f8-4b3e-b2cb-f4cf490efff8",
   "metadata": {},
   "source": [
    "### How does _Lasagna AI_ define an Agent?\n",
    "\n",
    "Everything above â˜ï¸ is too theoretical. When the rubber meets the road, what actually _is_ an **agent** inside _Lasagna AI_?\n",
    "\n",
    "In _Lasagna_, an **agent** is a unit of work that leverages a `Model`.\n",
    "\n",
    "Think of an agent as a specialized _worker_ that:\n",
    "\n",
    "1. **Analyzes** the current situation.\n",
    "2. **Decides** what needs to be done.\n",
    "3. **Acts** using AI models and tools.\n",
    "4. **Records** its output.\n",
    "\n",
    "In Lasagna, agents are _composable_. You begin by developing individual agents, each with a narrow focus, then you _compose_ them together into a complex multi-agent system. Like humans, it's helpful to decompose and delegate tasks amongst the group.\n",
    "\n",
    "In the next section, we'll see how to write **agents** using Lasagna AI's interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ont02120yu",
   "metadata": {},
   "source": [
    "## The Agent Interface\n",
    "\n",
    "Every Lasagna agent follows the same pattern â€” it's a _callable_ (either a function or a callable-object) that takes exactly **three parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tds0cpw956o",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def my_agent(\n",
    "    model: Model,                   # â† the AI model available to this agent\n",
    "    event_callback: EventCallback,  # â† used for streaming and event handling\n",
    "    prev_runs: list[AgentRun],      # â† previous work, context, or conversation history\n",
    ") -> AgentRun:\n",
    "    # Agent logic goes here...\n",
    "    raise RuntimeError('not yet implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7wrp2g8an",
   "metadata": {},
   "source": [
    "Let's understand what each parameter represents:\n",
    "\n",
    "- **`model`**: The AI model (like GPT-4o, Claude, etc.) that your agent can use for reasoning, text generation, or decision-making.\n",
    "- **`event_callback`**: A function for handling streaming events and progress updates. This enables real-time feedback as your agent works.\n",
    "- **`prev_runs`**: The history of previous work. In a conversation, this contains past messages. In a workflow, this contains results from earlier steps.\n",
    "\n",
    "The agent returns an `AgentRun` â€” a structured representation of what the agent generated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1651ee86-c49e-40d3-8f59-ce97983f0389",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## ðŸ“Œ Put a pin in `AgentRun`\n",
    "\n",
    "For now, just know that `AgentRun` is a core data type. We'll explore `AgentRun` in detail in the [next chapter](type_agentrun.ipynb)!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q37aac6nazp",
   "metadata": {},
   "source": [
    "## How do I write an agent?\n",
    "\n",
    "When you sit down to write an agent, here is what you must consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fc110-1597-481a-a28a-174d5ae6ae56",
   "metadata": {},
   "source": [
    "### 1. Analyze the Current Situation\n",
    "\n",
    "Your agent must _examine_ `prev_runs` to understand what has happened so far. It may find:\n",
    "\n",
    "- previous messages in a conversation,\n",
    "- results from earlier agents in a workflow, and/or\n",
    "- intermediate outputs from a multi-step process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eec636d-9b58-4fed-a32b-cf3e5211c3bb",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Analysis\" is Trivial\n",
    "\n",
    "It's common that an agent will _expect_ certain types of messages, so no real \"examination\" takes place. In those cases, the agent may simply `assert` what it expects.\n",
    "\n",
    "Otherwise, the agent is free to filter/clean/reformulate/branch-off `prev_runs` in any way you see fit!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34d9d9-55cf-4ce8-bfe5-1224cc74c5ce",
   "metadata": {},
   "source": [
    "### 2. Make Behavioral Decisions\n",
    "\n",
    "Your agent must _decide_ what to do next. It might:\n",
    "\n",
    "- generate a response using the AI `model`, or\n",
    "- use the AI `model` to extract data, or\n",
    "- pass tools to the AI `model`, or\n",
    "- split its task into multiple subtasks and delegate those to downstream agents, or\n",
    "- do _many_ of the things above as many times as it chooses!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22306bd5-daa5-4b62-adbd-7b804c8f6d91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Decision\" is Trivial\n",
    "\n",
    "It's common that an agent does _not_ \"decide\" anything on-the-fly; rather, you may write your agent to _always do_ one of the things above. However, you are free to write an agent that _does_ decide on-the-fly (perhaps via help from the AI `model` or a downstream agent), as you see fit.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0828a0-77e7-4116-a558-2fa972c61a5e",
   "metadata": {},
   "source": [
    "### 3. Take Action\n",
    "\n",
    "Your agent must _execute_ its decision:\n",
    "\n",
    "- **Model interaction**: Invoke the AI `model` to generate text, reason about problems, or extract structured outputs.\n",
    "- **Tool usage**: Send emails, query a database, etc.\n",
    "- **Agent delegation**: Invoke downstream agents to handle subtasks."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f718a423-82d0-4d4b-bd37-27e000bedf91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Good ol' Programming Time\n",
    "\n",
    "This is good-ol' hands-on-the-keyboard write-Python-code. This is you writing code to (1) invoke the AI `model` with the correct inputs, (2) grab the AI response and make use of it, (3) connect your agents together, and (4) connect your agents to the rest of your software stack. Get to it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ef052-5b4d-4777-a3ad-6b38869c0a5d",
   "metadata": {},
   "source": [
    "### 4. Record its Output\n",
    "\n",
    "Your agent must _construct_ and _return_ an `AgentRun` that contains:\n",
    "\n",
    "- any new information that it generated, and/or\n",
    "- results from sub-agents it coordinated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf0e6ca0-a3e3-4e6b-bb83-b603aabdd269",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## This is a critical step!\n",
    "\n",
    "The `AgentRun` you return here will be passed as input to other agents (or to your same agent, in the case of multi-turn chat). It's _critical_ that you record everything that happened and return it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmj16o92rw",
   "metadata": {},
   "source": [
    "## Real Agent Examples\n",
    "\n",
    "Let's look at some **real** examples to see agents in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0084a-1eee-493c-aa77-3b29c2be814a",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we write and run some agents, we need to set up our \"binder\" (see the [quickstart guide](../quickstart.ipynb) for what this is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24107132-fa62-4eef-b641-359a4e0b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if os.environ.get('OPENAI_API_KEY'):\n",
    "    print('Using OpenAI')\n",
    "    binder = known_models.openai_gpt_4o_binder\n",
    "\n",
    "elif os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    print('Using Anthropic')\n",
    "    binder = known_models.anthropic_claude_sonnet_4_binder\n",
    "\n",
    "else:\n",
    "    assert False, \"Neither OPENAI_API_KEY nor ANTHROPIC_API_KEY is set! We need at least one to do this demo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noblz7bden",
   "metadata": {},
   "source": [
    "### The Conversational Agent\n",
    "\n",
    "This is the simplest type of agent â€” it uses the message history to generate a new text response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a76b04-26e3-4296-8f50-e0391af3a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Use the model to generate a _new_ response:\n",
    "    new_messages = await model.run(event_callback, messages, tools=[])\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('chat_agent', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tnlh8nkl1o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi, my name is Ryan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHi\u001b[0m\u001b[0m Ryan\u001b[0m\u001b[0m!\u001b[0m\u001b[0m Nice\u001b[0m\u001b[0m to\u001b[0m\u001b[0m meet\u001b[0m\u001b[0m you\u001b[0m\u001b[0m.\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m help\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYour\u001b[0m\u001b[0m name\u001b[0m\u001b[0m is\u001b[0m\u001b[0m Ryan\u001b[0m\u001b[0m!\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(chat_agent))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6zm8snjnbwu",
   "metadata": {},
   "source": [
    "### The Specialist Agent\n",
    "\n",
    "Agents can be specialized for particular tasks. Here's an agent that focuses on providing helpful coding advice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb6728e-f852-4f5e-b533-1873a052a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODING_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful coding assistant.\n",
    "Provide clear, practical advice with code examples when appropriate.\n",
    "Focus on best practices and explain your reasoning.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac767793-50bf-4ecb-9267-233a51e0e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def coding_advisor(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Generate a response with an OVERRIDDEN system prompt!\n",
    "    modified_messages = override_system_prompt(messages, CODING_SYSTEM_PROMPT)\n",
    "    new_messages = await model.run(event_callback, modified_messages, tools=[])\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('coding_advisor', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "offgo6balkj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Who are you? (answer briefly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI\u001b[0m\u001b[0mâ€™m\u001b[0m\u001b[0m a\u001b[0m\u001b[0m coding\u001b[0m\u001b[0m assistant\u001b[0m\u001b[0m here\u001b[0m\u001b[0m to\u001b[0m\u001b[0m help\u001b[0m\u001b[0m you\u001b[0m\u001b[0m with\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m questions\u001b[0m\u001b[0m,\u001b[0m\u001b[0m code\u001b[0m\u001b[0m problems\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m learning\u001b[0m\u001b[0m best\u001b[0m\u001b[0m practices\u001b[0m\u001b[0m.\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m help\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How do I add numbers in Python? (answer in 1 sentence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mTo\u001b[0m\u001b[0m add\u001b[0m\u001b[0m numbers\u001b[0m\u001b[0m in\u001b[0m\u001b[0m Python\u001b[0m\u001b[0m,\u001b[0m\u001b[0m use\u001b[0m\u001b[0m the\u001b[0m\u001b[0m `\u001b[0m\u001b[0m+\u001b[0m\u001b[0m`\u001b[0m\u001b[0m operator\u001b[0m\u001b[0m,\u001b[0m\u001b[0m like\u001b[0m\u001b[0m `\u001b[0m\u001b[0mresult\u001b[0m\u001b[0m =\u001b[0m\u001b[0m num\u001b[0m\u001b[0m1\u001b[0m\u001b[0m +\u001b[0m\u001b[0m num\u001b[0m\u001b[0m2\u001b[0m\u001b[0m`.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(coding_advisor))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24607c-e56e-485d-a0c5-bed4ff758696",
   "metadata": {},
   "source": [
    "### The Information Extractor\n",
    "\n",
    "Let's make an agent that does **structured output** to _extract_ information from the user's message. In particular, we'll have this agent classify the user's message (i.e. it is \"extracting\" the classification, if you will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf224901-b74f-49a5-95e2-074c14b6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_CLASSIFIER_SYSTEM_PROMPT = \"\"\"\n",
    "Your job is to classify the user's message into one of the following categories:\n",
    " - `small_talk`: Comments like \"hi\", \"how are you?\", etc.\n",
    " - `programming`: Questions or comments about programming languages, libraries, etc.\n",
    " - `other`: Any message that is not small talk and not programming.\n",
    "\"\"\".strip()\n",
    "\n",
    "# In a production-grade system, you'd probably expand your system prompt to\n",
    "# be more thorough; we're going for minimal here to keep this demo short.\n",
    "\n",
    "class Category(Enum):\n",
    "    small_talk = 'small_talk'\n",
    "    programming = 'programming'\n",
    "    other = 'other'\n",
    "\n",
    "class CategoryOutput(BaseModel):\n",
    "    thoughts: str\n",
    "    category: Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b9bee6-9795-43c4-8375-5b00ae442c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def intent_classifier(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Get **ONLY** the last message from the conversation so far:\n",
    "    #    (Just for demo-purposes, to show you can do whatever you want with\n",
    "    #     `prev_runs` ðŸ˜. A production-grade intent classifier would consider\n",
    "    #     more than just the last message.)\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # Generate a structured output response with an OVERRIDDEN system prompt!\n",
    "    messages = override_system_prompt([last_message], INTENT_CLASSIFIER_SYSTEM_PROMPT)\n",
    "    new_message, result = await model.extract(event_callback, messages, CategoryOutput)\n",
    "    assert isinstance(result, CategoryOutput)\n",
    "\n",
    "    # Wrap the new messages into an `AgentRun` result:\n",
    "    return extraction('intent_classifier', [new_message], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8843b12-5cb7-4ed3-acef-cbf16ae087f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m is\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m with\u001b[0m\u001b[31m a\u001b[0m\u001b[31m simple\u001b[0m\u001b[31m '\u001b[0m\u001b[31mHi\u001b[0m\u001b[31m!',\u001b[0m\u001b[31m which\u001b[0m\u001b[31m is\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Sup?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m message\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m casual\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m is\u001b[0m\u001b[31m asking\u001b[0m\u001b[31m about\u001b[0m\u001b[31m Python\u001b[0m\u001b[31m,\u001b[0m\u001b[31m which\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m language\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mprogram\u001b[0m\u001b[31mming\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m is\u001b[0m\u001b[31m asking\u001b[0m\u001b[31m a\u001b[0m\u001b[31m basic\u001b[0m\u001b[31m math\u001b[0m\u001b[31m question\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mother\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(intent_classifier))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adebd9d-959c-4d6d-8863-04e8f9739c4c",
   "metadata": {},
   "source": [
    "### The 'Back on Track' Agent\n",
    "\n",
    "This agent is pretty useless on its own, but you'll see soon why we're creating it. It just tells the user to get back on track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5caf5a42-f514-4b34-80e3-55caa4dac78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_ON_TRACK_SYSTEM_PROMPT = \"\"\"\n",
    "The user's message has been deemed to be off-topic.\n",
    "Please politely tell them that their message is off-topic.\n",
    "Do not respond to their question or their request. Just politely\n",
    "tell them they are off-topic and need to return to the topic\n",
    "at-hand.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430071f2-60b4-4d0d-a3d1-6e952b46633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def back_on_track(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    messages = override_system_prompt(\n",
    "        [last_message],\n",
    "        BACK_ON_TRACK_SYSTEM_PROMPT,\n",
    "    )\n",
    "    return flat_messages(\n",
    "        'back_on_track',\n",
    "        await model.run(event_callback, messages, tools=[]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfa7ba1-ac85-45a1-9952-4d4c96c5183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m It\u001b[0m\u001b[0m seems\u001b[0m\u001b[0m your\u001b[0m\u001b[0m message\u001b[0m\u001b[0m is\u001b[0m\u001b[0m off\u001b[0m\u001b[0m-topic\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Let's\u001b[0m\u001b[0m return\u001b[0m\u001b[0m to\u001b[0m\u001b[0m the\u001b[0m\u001b[0m topic\u001b[0m\u001b[0m at\u001b[0m\u001b[0m hand\u001b[0m\u001b[0m.\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m with\u001b[0m\u001b[0m that\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(back_on_track))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a120-96be-4b0a-a593-562304c314c0",
   "metadata": {},
   "source": [
    "### The Routing Agent\n",
    "\n",
    "Now, we'll put all the pieces together by making a **routing agent** that uses **all four** agents above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdb0e673-a73c-477c-8e5b-07f842c34baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def router(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # Decide which downstream agent to use, based on the user's intent:\n",
    "    bound_intent_classifier = binder(intent_classifier)\n",
    "    classification_run = await bound_intent_classifier(event_callback, prev_runs)\n",
    "    assert classification_run['type'] == 'extraction'\n",
    "    classification_result = classification_run['result']\n",
    "    assert isinstance(classification_result, CategoryOutput)\n",
    "    if classification_result.category == Category.small_talk:\n",
    "        downstream_agent = binder(chat_agent)\n",
    "    elif classification_result.category == Category.programming:\n",
    "        downstream_agent = binder(coding_advisor)\n",
    "    else:\n",
    "        downstream_agent = binder(back_on_track)\n",
    "\n",
    "    # Delegate to the downstream agent!\n",
    "    downstream_run = await downstream_agent(event_callback, prev_runs)\n",
    "\n",
    "    # Wrap *everything* that happened above into the return:\n",
    "    return chained_runs('router', [classification_run, downstream_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd2ba67-1c00-4ce5-be97-b62bdc4db220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m just\u001b[0m\u001b[31m greeted\u001b[0m\u001b[31m me\u001b[0m\u001b[31m,\u001b[0m\u001b[31m which\u001b[0m\u001b[31m is\u001b[0m\u001b[31m considered\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python? (answer briefly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m question\u001b[0m\u001b[31m is\u001b[0m\u001b[31m directly\u001b[0m\u001b[31m about\u001b[0m\u001b[31m Python\u001b[0m\u001b[31m,\u001b[0m\u001b[31m a\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m language\u001b[0m\u001b[31m,\u001b[0m\u001b[31m so\u001b[0m\u001b[31m it\u001b[0m\u001b[31m fits\u001b[0m\u001b[31m the\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m category\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mprogram\u001b[0m\u001b[31mming\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mPython\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m high\u001b[0m\u001b[0m-level\u001b[0m\u001b[0m,\u001b[0m\u001b[0m general\u001b[0m\u001b[0m-purpose\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m language\u001b[0m\u001b[0m known\u001b[0m\u001b[0m for\u001b[0m\u001b[0m its\u001b[0m\u001b[0m simplicity\u001b[0m\u001b[0m,\u001b[0m\u001b[0m readability\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m versatility\u001b[0m\u001b[0m.\u001b[0m\u001b[0m It\u001b[0m\u001b[0m supports\u001b[0m\u001b[0m multiple\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m paradig\u001b[0m\u001b[0mms\u001b[0m\u001b[0m,\u001b[0m\u001b[0m including\u001b[0m\u001b[0m procedural\u001b[0m\u001b[0m,\u001b[0m\u001b[0m object\u001b[0m\u001b[0m-oriented\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m functional\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Python\u001b[0m\u001b[0m is\u001b[0m\u001b[0m widely\u001b[0m\u001b[0m used\u001b[0m\u001b[0m for\u001b[0m\u001b[0m web\u001b[0m\u001b[0m development\u001b[0m\u001b[0m,\u001b[0m\u001b[0m data\u001b[0m\u001b[0m science\u001b[0m\u001b[0m,\u001b[0m\u001b[0m artificial\u001b[0m\u001b[0m intelligence\u001b[0m\u001b[0m,\u001b[0m\u001b[0m automation\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m much\u001b[0m\u001b[0m more\u001b[0m\u001b[0m,\u001b[0m\u001b[0m thanks\u001b[0m\u001b[0m to\u001b[0m\u001b[0m its\u001b[0m\u001b[0m extensive\u001b[0m\u001b[0m standard\u001b[0m\u001b[0m library\u001b[0m\u001b[0m and\u001b[0m\u001b[0m vibrant\u001b[0m\u001b[0m ecosystem\u001b[0m\u001b[0m of\u001b[0m\u001b[0m third\u001b[0m\u001b[0m-party\u001b[0m\u001b[0m packages\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m message\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m general\u001b[0m\u001b[31m,\u001b[0m\u001b[31m non\u001b[0m\u001b[31m-program\u001b[0m\u001b[31mming\u001b[0m\u001b[31m question\u001b[0m\u001b[31m related\u001b[0m\u001b[31m to\u001b[0m\u001b[31m arithmetic\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mother\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m sorry\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m your\u001b[0m\u001b[0m message\u001b[0m\u001b[0m is\u001b[0m\u001b[0m off\u001b[0m\u001b[0m-topic\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Please\u001b[0m\u001b[0m return\u001b[0m\u001b[0m to\u001b[0m\u001b[0m the\u001b[0m\u001b[0m topic\u001b[0m\u001b[0m at\u001b[0m\u001b[0m hand\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How are you today? (also what is 2+2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m message\u001b[0m\u001b[31m contains\u001b[0m\u001b[31m a\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m element\u001b[0m\u001b[31m but\u001b[0m\u001b[31m also\u001b[0m\u001b[31m includes\u001b[0m\u001b[31m a\u001b[0m\u001b[31m simple\u001b[0m\u001b[31m math\u001b[0m\u001b[31m question\u001b[0m\u001b[31m which\u001b[0m\u001b[31m doesn't\u001b[0m\u001b[31m relate\u001b[0m\u001b[31m to\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m just\u001b[0m\u001b[0m a\u001b[0m\u001b[0m program\u001b[0m\u001b[0m,\u001b[0m\u001b[0m so\u001b[0m\u001b[0m I\u001b[0m\u001b[0m don\u001b[0m\u001b[0mâ€™t\u001b[0m\u001b[0m have\u001b[0m\u001b[0m feelings\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m thank\u001b[0m\u001b[0m you\u001b[0m\u001b[0m for\u001b[0m\u001b[0m asking\u001b[0m\u001b[0m!\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m As\u001b[0m\u001b[0m for\u001b[0m\u001b[0m your\u001b[0m\u001b[0m question\u001b[0m\u001b[0m:\u001b[0m\u001b[0m  \n",
      "\u001b[0m\u001b[0m**\u001b[0m\u001b[0m2\u001b[0m\u001b[0m +\u001b[0m\u001b[0m \u001b[0m\u001b[0m2\u001b[0m\u001b[0m =\u001b[0m\u001b[0m \u001b[0m\u001b[0m4\u001b[0m\u001b[0m**\u001b[0m\u001b[0m!\u001b[0m\u001b[0m ðŸŽ‰\u001b[0m\u001b[0m Let\u001b[0m\u001b[0m me\u001b[0m\u001b[0m know\u001b[0m\u001b[0m if\u001b[0m\u001b[0m you\u001b[0m\u001b[0m have\u001b[0m\u001b[0m more\u001b[0m\u001b[0m questions\u001b[0m\u001b[0m!\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(router))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2b8d2f0-5c19-4bcf-ad52-02777e17478d",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## ðŸš¨ Hacked!\n",
    "\n",
    "Did you notice above how we prompt-hacked our simple AI system? **YIKES!** (Look closely, we were able to get it to answer our `2 + 2` question on the second attempt above, by \"hiding\" the question in a small-talk message.)\n",
    "\n",
    "This is a good time to call out that AI models can (and will!) make mistakes. Prompt engineering helps, but even the most well-tuned prompting cannot protect your AI system from malicious users.\n",
    "\n",
    "Design your AI systems accordingly, and consult best-practice literature. The most important thing: Design your AI system so that it does no damage even _if_ (or _when_) it misbehaves.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115869f-274f-4eac-b32c-e3bf5cbacf8a",
   "metadata": {},
   "source": [
    "### The Task Splitter\n",
    "\n",
    "Another common task is for an agent to split work and delegate to multiple downstream agents. Let's do that next!\n",
    "\n",
    "We'll use a silly example, for simplicity and brevity, where we'll split the user's message into individual sentences, then prompt an AI `model` one-at-a-time on each individual sentence. While this is a silly example, it shows how you can split up a problem for multiple downstream subagents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1721848-54a8-4ffd-b9a7-7887b1e13a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def splitter(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # We'll only look at the most recent message:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    assert last_message['role'] == 'human'\n",
    "    assert last_message['text']\n",
    "\n",
    "    # We'll split the most recent message into sentences:\n",
    "    #    (This is **not** a robust way to do it, but we're keeping the demo simple.)\n",
    "    sentences = re.split(r'[\\.\\?\\!] ', last_message['text'])\n",
    "    sentences_as_agentruns = [\n",
    "        flat_messages('splitter', [{\n",
    "            'role': 'human',\n",
    "            'text': sentence,\n",
    "        }])\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    # Have the `chat_agent` respond to each sentence:\n",
    "    #    (Again, not particularly useful, but good for a brief demo.)\n",
    "    bound_chat_agent = binder(chat_agent)\n",
    "    downstream_runs: list[AgentRun] = []\n",
    "    for task_input in sentences_as_agentruns:\n",
    "        this_run = await bound_chat_agent(event_callback, [task_input])\n",
    "        downstream_runs.append(this_run)\n",
    "\n",
    "    # Wrap *everything* that happened above into the return:\n",
    "    return parallel_runs('splitter', downstream_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "974bc078-75fd-4958-ade4-d46b14cf0ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi. What's up? How are you? What's 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mNot\u001b[0m\u001b[0m much\u001b[0m\u001b[0m,\u001b[0m\u001b[0m just\u001b[0m\u001b[0m here\u001b[0m\u001b[0m and\u001b[0m\u001b[0m ready\u001b[0m\u001b[0m to\u001b[0m\u001b[0m help\u001b[0m\u001b[0m!\u001b[0m\u001b[0m What's\u001b[0m\u001b[0m on\u001b[0m\u001b[0m your\u001b[0m\u001b[0m mind\u001b[0m\u001b[0m?\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m just\u001b[0m\u001b[0m a\u001b[0m\u001b[0m virtual\u001b[0m\u001b[0m assistant\u001b[0m\u001b[0m,\u001b[0m\u001b[0m so\u001b[0m\u001b[0m I\u001b[0m\u001b[0m don't\u001b[0m\u001b[0m have\u001b[0m\u001b[0m feelings\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m thanks\u001b[0m\u001b[0m for\u001b[0m\u001b[0m asking\u001b[0m\u001b[0m!\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m2\u001b[0m\u001b[0m +\u001b[0m\u001b[0m \u001b[0m\u001b[0m2\u001b[0m\u001b[0m =\u001b[0m\u001b[0m \u001b[0m\u001b[0m4\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Thanks! What did I just say?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYou're\u001b[0m\u001b[0m welcome\u001b[0m\u001b[0m!\u001b[0m\u001b[0m Do\u001b[0m\u001b[0m you\u001b[0m\u001b[0m need\u001b[0m\u001b[0m help\u001b[0m\u001b[0m with\u001b[0m\u001b[0m anything\u001b[0m\u001b[0m?\u001b[0m\u001b[0m ðŸ˜Š\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI\u001b[0m\u001b[0m can't\u001b[0m\u001b[0m recall\u001b[0m\u001b[0m or\u001b[0m\u001b[0m access\u001b[0m\u001b[0m past\u001b[0m\u001b[0m interactions\u001b[0m\u001b[0m in\u001b[0m\u001b[0m this\u001b[0m\u001b[0m chat\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Could\u001b[0m\u001b[0m you\u001b[0m\u001b[0m please\u001b[0m\u001b[0m repeat\u001b[0m\u001b[0m or\u001b[0m\u001b[0m clarify\u001b[0m\u001b[0m what\u001b[0m\u001b[0m you\u001b[0m\u001b[0m said\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(splitter))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd9414fb-c54c-41a3-a25a-b24f1d67cc67",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## What happened?\n",
    "\n",
    "In the conversation above, when I said \"What did I just say?\", the AI model didn't _see_ the conversation history. Why not? It's because of how we wrote our agent â€” we did not pass the whole conversation to the AI model!\n",
    "\n",
    "**Exercise for the reader:** How can you change the agent so that the AI _sees_ the whole conversation history?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca62ce9-3346-4f03-8260-f3d34f7b6cd6",
   "metadata": {},
   "source": [
    "### Do anything!\n",
    "\n",
    "It's up to you how to write your multi-agent AI system. You can mix-and-match ideas, include lots of behaviors in a _single_ agent, or split up tasks among _multiple_ agents. You can have \"meta agents\" that plan work for other agents, or \"meta meta agents\" that plan work for your \"meta agents\". As long as it is _safe_ and _works_, go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8hujw6nu5o",
   "metadata": {},
   "source": [
    "## Why This Design?\n",
    "\n",
    "Lasagna's agent design provides several key benefits:\n",
    "\n",
    "### ðŸ”Œ **Pluggability** \n",
    "\n",
    "Every agent follows the same interface, so you can:\n",
    "\n",
    "- swap one agent for another,\n",
    "- combine agents from different sources, and\n",
    "- test agents in isolation.\n",
    "\n",
    "### ðŸ¥ž **Layering**\n",
    "\n",
    "You can compose agents at any level:\n",
    "\n",
    "- Use simple agents as building blocks.\n",
    "- Combine them into more complex workflows.\n",
    "- Build entire systems from agent compositions.\n",
    "\n",
    "### ðŸ”„ **Reusability**\n",
    "\n",
    "Write an agent once, use it everywhere:\n",
    "\n",
    "- as a standalone agent,\n",
    "- as part of a larger workflow, or\n",
    "- as a specialist in a multi-agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7yu95a7mrk",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand what agents are and how they work conceptually, you're ready to dive deeper into the technical details.\n",
    "\n",
    "In the [next section](type_agentrun.ipynb), we'll explore the `AgentRun` data structure in detail â€” the standardized format that enables all this agent composition and layering.\n",
    "\n",
    "You'll learn about:\n",
    "\n",
    "- The four types of `AgentRun`.\n",
    "- How to work with the recursive data structure.\n",
    "- Helper functions for common patterns.\n",
    "- Advanced features like cost tracking and serialization.\n",
    "\n",
    "For more advanced agent patterns and real-world examples, check out:\n",
    "\n",
    "- [Tool Use](../agent_features/tools.ipynb) â€” Agents that interact with external systems\n",
    "- [Structured Output](../agent_features/structured_output.ipynb) â€” Agents that extract structured data\n",
    "- [Layering](../agent_features/layering.ipynb) â€” Complex multi-agent compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ba306-7f00-4953-b96e-143de89597c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
