{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d6734406-dc8b-4727-be3e-9e2055385035",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"The Lasagna `Agent`\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81da2c4-2109-4a56-b157-bd53383903b5",
   "metadata": {},
   "source": [
    "In Lasagna AI, an **Agent** is a unit of AI-powered reasoning that performs _specific_ work. Agents are the building blocks of your AI system — you compose simple agents together to create powerful multi-agent workflows.\n",
    "\n",
    "Think of an agent as a specialized _worker_ that:\n",
    "\n",
    "1. **Analyzes** the current situation.\n",
    "2. **Decides** what needs to be done.\n",
    "3. **Acts** using AI models and tools.\n",
    "4. **Records** its output.\n",
    "\n",
    "The beauty of Lasagna's approach is that agents are **composable**. You begin by developing individual agents, each with a narrow focus, then you compose them together into a complex multi-agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mqyo5n16yb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This page will use the following imports:\n",
    "\n",
    "from lasagna import Model, EventCallback, AgentRun\n",
    "from lasagna import (\n",
    "    recursive_extract_messages,\n",
    "    extract_last_message,\n",
    "    override_system_prompt,\n",
    "    flat_messages,\n",
    "    parallel_runs,\n",
    "    chained_runs,\n",
    "    extraction,\n",
    ")\n",
    "from lasagna import known_models\n",
    "from lasagna.tui import tui_input_loop\n",
    "\n",
    "import os\n",
    "import re\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ont02120yu",
   "metadata": {},
   "source": [
    "## The Agent Interface\n",
    "\n",
    "Every Lasagna agent follows the same pattern — it's a _callable_ (either a function or a callable-object) that takes exactly **three parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tds0cpw956o",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def my_agent(\n",
    "    model: Model,                   # ← the AI model available to this agent\n",
    "    event_callback: EventCallback,  # ← used for streaming and event handling\n",
    "    prev_runs: list[AgentRun],      # ← previous work, context, or conversation history\n",
    ") -> AgentRun:\n",
    "    # Agent logic goes here...\n",
    "    raise RuntimeError('not yet implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7wrp2g8an",
   "metadata": {},
   "source": [
    "Let's understand what each parameter represents:\n",
    "\n",
    "- **`model`**: The AI model (like GPT-4o, Claude, etc.) that your agent can use for reasoning, text generation, or decision-making.\n",
    "- **`event_callback`**: A function for handling streaming events and progress updates. This enables real-time feedback as your agent works.\n",
    "- **`prev_runs`**: The history of previous work. In a conversation, this contains past messages. In a workflow, this contains results from earlier steps.\n",
    "\n",
    "The agent returns an `AgentRun` — a structured representation of what the agent generated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1651ee86-c49e-40d3-8f59-ce97983f0389",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## 📌 Put a pin in `AgentRun`\n",
    "\n",
    "For now, just know that `AgentRun` is a core data type. We'll explore `AgentRun` in detail in the [next chapter](type_agentrun.ipynb)!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q37aac6nazp",
   "metadata": {},
   "source": [
    "## How do I write an agent?\n",
    "\n",
    "When you sit down to write an agent, here is what you must consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fc110-1597-481a-a28a-174d5ae6ae56",
   "metadata": {},
   "source": [
    "### 1. Analyze the Current Situation\n",
    "\n",
    "Your agent must _examine_ `prev_runs` to understand what has happened so far. It may find:\n",
    "\n",
    "- previous messages in a conversation,\n",
    "- results from earlier agents in a workflow, and/or\n",
    "- intermediate outputs from a multi-step process."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eec636d-9b58-4fed-a32b-cf3e5211c3bb",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Analysis\" is Trivial\n",
    "\n",
    "It's common that an agent will _expect_ certain types of messages, so no real \"examination\" takes place. In those cases, the agent may simply `assert` what it expects.\n",
    "\n",
    "Otherwise, the agent is free to filter/clean/reformulate/branch-off `prev_runs` in any way you see fit!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34d9d9-55cf-4ce8-bfe5-1224cc74c5ce",
   "metadata": {},
   "source": [
    "### 2. Make Behavioral Decisions\n",
    "\n",
    "Your agent must _decide_ what to do next. It might:\n",
    "\n",
    "- generate a response using the AI `model`, or\n",
    "- use the AI `model` to extract data, or\n",
    "- pass tools to the AI `model`, or\n",
    "- split its task into multiple subtasks and delegate those to downstream agents, or\n",
    "- do _many_ of the things above as many times as it chooses!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22306bd5-daa5-4b62-adbd-7b804c8f6d91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Sometimes the \"Decision\" is Trivial\n",
    "\n",
    "It's common that an agent does _not_ \"decide\" anything on-the-fly; rather, you may write your agent to _always do_ one of the things above. However, you are free to write an agent that _does_ decide on-the-fly (perhaps via help from the AI `model` or a downstream agent), as you see fit.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0828a0-77e7-4116-a558-2fa972c61a5e",
   "metadata": {},
   "source": [
    "### 3. Take Action\n",
    "\n",
    "Your agent must _execute_ its decision:\n",
    "\n",
    "- **Model interaction**: Invokes the AI `model` to generate text, reason about problems, or extract structured outputs.\n",
    "- **Tool usage**: Sends emails, queries a database, etc.\n",
    "- **Agent delegation**: Invokes downstream agents to handle subtasks."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f718a423-82d0-4d4b-bd37-27e000bedf91",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Good ol' Programming Time\n",
    "\n",
    "This is good-ol' hands-on-the-keyboard write-Python-code. This is you writing code to (1) invoke the AI `model` with the correct inputs, (2) grab the AI response and make use of it, (3) connect your agents together, and (4) connect your agents to the rest of your software stack. Get to it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ef052-5b4d-4777-a3ad-6b38869c0a5d",
   "metadata": {},
   "source": [
    "### 4. Record its Output\n",
    "\n",
    "Your agent must _construct_ and _return_ an `AgentRun` that contains:\n",
    "\n",
    "- any new information that it generated, and/or\n",
    "- results from sub-agents it coordinated."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf0e6ca0-a3e3-4e6b-bb83-b603aabdd269",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## This is a critical step!\n",
    "\n",
    "The `AgentRun` you return here will be passed as input to other agents (or to your same agent, in the case of multi-turn chat). It's _critical_ you record everything that happened and return it!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmj16o92rw",
   "metadata": {},
   "source": [
    "## Real Agent Examples\n",
    "\n",
    "Let's look at some **real** examples to see agents in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0084a-1eee-493c-aa77-3b29c2be814a",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Before we write and run some agents, we need to set up our \"binder\" (see the [quickstart guide](../quickstart.ipynb) for what this is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24107132-fa62-4eef-b641-359a4e0b1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if os.environ.get('OPENAI_API_KEY'):\n",
    "    print('Using OpenAI')\n",
    "    binder = known_models.BIND_OPENAI_gpt_4o()\n",
    "\n",
    "elif os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    print('Using Anthropic')\n",
    "    binder = known_models.BIND_ANTHROPIC_claude_sonnet_4()\n",
    "\n",
    "else:\n",
    "    assert False, \"Neither OPENAI_API_KEY nor ANTHROPIC_API_KEY is set! We need at least one to do this demo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noblz7bden",
   "metadata": {},
   "source": [
    "### The Conversational Agent\n",
    "\n",
    "This is the simplest type of agent — it uses the message history to generate a new text response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a76b04-26e3-4296-8f50-e0391af3a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # 1. Analyze: Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # 2. Decide: This agent doesn't \"decide\" anything; its behavior is static.\n",
    "\n",
    "    # 3. Act: Use the model to generate a response:\n",
    "    new_messages = await model.run(event_callback, messages, tools=[])\n",
    "\n",
    "    # 4. Record: Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('chat_agent', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tnlh8nkl1o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi, my name is Ryan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHi\u001b[0m\u001b[0m Ryan\u001b[0m\u001b[0m!\u001b[0m\u001b[0m Nice\u001b[0m\u001b[0m to\u001b[0m\u001b[0m meet\u001b[0m\u001b[0m you\u001b[0m\u001b[0m.\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m help\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYour\u001b[0m\u001b[0m name\u001b[0m\u001b[0m is\u001b[0m\u001b[0m Ryan\u001b[0m\u001b[0m!\u001b[0m\u001b[0m 😊\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(chat_agent))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6zm8snjnbwu",
   "metadata": {},
   "source": [
    "### The Specialist Agent\n",
    "\n",
    "Agents can be specialized for particular tasks. Here's an agent that focuses on providing helpful coding advice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb6728e-f852-4f5e-b533-1873a052a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODING_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful coding assistant.\n",
    "Provide clear, practical advice with code examples when appropriate.\n",
    "Focus on best practices and explain your reasoning.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac767793-50bf-4ecb-9267-233a51e0e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def coding_advisor(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # 1. Analyze: Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # 2. Decide: This agent doesn't \"decide\" anything; its behavior is static.\n",
    "\n",
    "    # 3. Act: Generate a response with an OVERRIDDEN system prompt!\n",
    "    modified_messages = override_system_prompt(messages, CODING_SYSTEM_PROMPT)\n",
    "    new_messages = await model.run(event_callback, modified_messages, tools=[])\n",
    "\n",
    "    # 4. Record: Wrap the new messages into an `AgentRun` result:\n",
    "    return flat_messages('coding_advisor', new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "offgo6balkj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Who are you? (answer briefly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m a\u001b[0m\u001b[0m coding\u001b[0m\u001b[0m assistant\u001b[0m\u001b[0m designed\u001b[0m\u001b[0m to\u001b[0m\u001b[0m help\u001b[0m\u001b[0m you\u001b[0m\u001b[0m solve\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m challenges\u001b[0m\u001b[0m,\u001b[0m\u001b[0m explain\u001b[0m\u001b[0m concepts\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m provide\u001b[0m\u001b[0m best\u001b[0m\u001b[0m practices\u001b[0m\u001b[0m for\u001b[0m\u001b[0m writing\u001b[0m\u001b[0m code\u001b[0m\u001b[0m.\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m help\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How do I add numbers in Python? (answer in 1 sentence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYou\u001b[0m\u001b[0m can\u001b[0m\u001b[0m add\u001b[0m\u001b[0m numbers\u001b[0m\u001b[0m in\u001b[0m\u001b[0m Python\u001b[0m\u001b[0m using\u001b[0m\u001b[0m the\u001b[0m\u001b[0m `\u001b[0m\u001b[0m+\u001b[0m\u001b[0m`\u001b[0m\u001b[0m operator\u001b[0m\u001b[0m,\u001b[0m\u001b[0m for\u001b[0m\u001b[0m example\u001b[0m\u001b[0m:\u001b[0m\u001b[0m `\u001b[0m\u001b[0mresult\u001b[0m\u001b[0m =\u001b[0m\u001b[0m \u001b[0m\u001b[0m5\u001b[0m\u001b[0m +\u001b[0m\u001b[0m \u001b[0m\u001b[0m3\u001b[0m\u001b[0m`.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(coding_advisor))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24607c-e56e-485d-a0c5-bed4ff758696",
   "metadata": {},
   "source": [
    "### The Information Extractor\n",
    "\n",
    "Let's make an agent that does structured output to extract information from the user's message. In particular, we'll have this agent classify the user's message (i.e. it is \"extracting\" the classification, if you will)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf224901-b74f-49a5-95e2-074c14b6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_CLASSIFIER_SYSTEM_PROMPT = \"\"\"\n",
    "Your job is to classify the user's message into one of the following categories:\n",
    " - `small_talk`: Comments like \"hi\", \"how are you?\", etc.\n",
    " - `programming`: Questions or comments about programming languages, libraries, etc.\n",
    " - `other`: Any message that is not small talk and not programming.\n",
    "\"\"\".strip()\n",
    "\n",
    "# In a production-grade system, you'd probably expand your system prompt to\n",
    "# be more thorough; we're going for minimal here to keep this demo short.\n",
    "\n",
    "class Category(Enum):\n",
    "    small_talk = 'small_talk'\n",
    "    programming = 'programming'\n",
    "    other = 'other'\n",
    "\n",
    "class CategoryOutput(BaseModel):\n",
    "    thoughts: str\n",
    "    category: Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b9bee6-9795-43c4-8375-5b00ae442c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def intent_classifier(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # 1. Analyze: Get **ONLY** the last message from the conversation so far:\n",
    "    #    (Just for demo-purposes, to show you can do whatever you want with\n",
    "    #     `prev_runs` 😁. A production-grade intent classifier would consider\n",
    "    #     more than just the last message.)\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # 2. Decide: This agent doesn't \"decide\" anything; its behavior is static.\n",
    "\n",
    "    # 3. Act: Generate a structured output response with an OVERRIDDEN system prompt!\n",
    "    messages = override_system_prompt([last_message], INTENT_CLASSIFIER_SYSTEM_PROMPT)\n",
    "    new_message, result = await model.extract(event_callback, messages, CategoryOutput)\n",
    "\n",
    "    # 4. Record: Wrap the new messages into an `AgentRun` result:\n",
    "    return extraction('intent_classifier', [new_message], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8843b12-5cb7-4ed3-acef-cbf16ae087f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThis\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m casual\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Sup?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user's\u001b[0m\u001b[31m message\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m casual\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m seems\u001b[0m\u001b[31m to\u001b[0m\u001b[31m be\u001b[0m\u001b[31m asking\u001b[0m\u001b[31m about\u001b[0m\u001b[31m Python\u001b[0m\u001b[31m as\u001b[0m\u001b[31m a\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m language\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mprogram\u001b[0m\u001b[31mming\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m is\u001b[0m\u001b[31m asking\u001b[0m\u001b[31m a\u001b[0m\u001b[31m simple\u001b[0m\u001b[31m math\u001b[0m\u001b[31m question\u001b[0m\u001b[31m,\u001b[0m\u001b[31m which\u001b[0m\u001b[31m doesn't\u001b[0m\u001b[31m fall\u001b[0m\u001b[31m under\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m or\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mother\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(intent_classifier))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adebd9d-959c-4d6d-8863-04e8f9739c4c",
   "metadata": {},
   "source": [
    "### The 'Back on Track' Agent\n",
    "\n",
    "This agent is pretty useless on its own, but you'll see soon why we're creating it. It just tells the user to get back on track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5caf5a42-f514-4b34-80e3-55caa4dac78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACK_ON_TRACK_SYSTEM_PROMPT = \"\"\"\n",
    "The user's message has been deemed to be off-topic.\n",
    "Please politely tell them that their message is off-topic.\n",
    "Do not respond to their question or their request. Just politely\n",
    "tell them they are off-topic and need to return to the topic\n",
    "at-hand.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430071f2-60b4-4d0d-a3d1-6e952b46633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def back_on_track(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    messages = override_system_prompt(\n",
    "        [last_message],\n",
    "        BACK_ON_TRACK_SYSTEM_PROMPT,\n",
    "    )\n",
    "    return flat_messages(\n",
    "        'back_on_track',\n",
    "        await model.run(event_callback, messages, tools=[]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfa7ba1-ac85-45a1-9952-4d4c96c5183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m It\u001b[0m\u001b[0m seems\u001b[0m\u001b[0m your\u001b[0m\u001b[0m message\u001b[0m\u001b[0m is\u001b[0m\u001b[0m off\u001b[0m\u001b[0m-topic\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Please\u001b[0m\u001b[0m return\u001b[0m\u001b[0m to\u001b[0m\u001b[0m the\u001b[0m\u001b[0m topic\u001b[0m\u001b[0m at\u001b[0m\u001b[0m hand\u001b[0m\u001b[0m so\u001b[0m\u001b[0m I\u001b[0m\u001b[0m can\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m effectively\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Thank\u001b[0m\u001b[0m you\u001b[0m\u001b[0m!\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m sorry\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m that\u001b[0m\u001b[0m question\u001b[0m\u001b[0m is\u001b[0m\u001b[0m off\u001b[0m\u001b[0m-topic\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Please\u001b[0m\u001b[0m return\u001b[0m\u001b[0m to\u001b[0m\u001b[0m the\u001b[0m\u001b[0m topic\u001b[0m\u001b[0m at\u001b[0m\u001b[0m hand\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(back_on_track))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a120-96be-4b0a-a593-562304c314c0",
   "metadata": {},
   "source": [
    "### The Routing Agent\n",
    "\n",
    "Now, we'll put all the pieces together by making a **routing agent** that uses **all four** agents above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb0e673-a73c-477c-8e5b-07f842c34baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def router(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # 1. Analyze: Extract all previous messages from the conversation:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "\n",
    "    # 2. Decide: This is the first agent to \"decide\" something! It asks the intent classifier, and will act accordingly:\n",
    "    classification_run = await binder(intent_classifier)(event_callback, prev_runs)\n",
    "    assert classification_run['type'] == 'extraction'\n",
    "    classification_result = classification_run['result']\n",
    "    assert isinstance(classification_result, CategoryOutput)\n",
    "    if classification_result.category == Category.small_talk:\n",
    "        downstream_agent = chat_agent\n",
    "    elif classification_result.category == Category.programming:\n",
    "        downstream_agent = coding_advisor\n",
    "    else:\n",
    "        downstream_agent = back_on_track\n",
    "\n",
    "    # 3. Act: Delegate to the downstream agent!\n",
    "    downstream_run = await binder(downstream_agent)(event_callback, prev_runs)\n",
    "\n",
    "    # 4. Record: Wrap *everything* that happened above into the return:\n",
    "    return chained_runs('router', [classification_run, downstream_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd2ba67-1c00-4ce5-be97-b62bdc4db220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m message\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m greeting\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m 😊\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is Python? (answer briefly)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThe\u001b[0m\u001b[31m user\u001b[0m\u001b[31m is\u001b[0m\u001b[31m asking\u001b[0m\u001b[31m about\u001b[0m\u001b[31m Python\u001b[0m\u001b[31m,\u001b[0m\u001b[31m which\u001b[0m\u001b[31m is\u001b[0m\u001b[31m a\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m language\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mprogram\u001b[0m\u001b[31mming\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mPython\u001b[0m\u001b[0m is\u001b[0m\u001b[0m a\u001b[0m\u001b[0m high\u001b[0m\u001b[0m-level\u001b[0m\u001b[0m,\u001b[0m\u001b[0m versatile\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m widely\u001b[0m\u001b[0m-used\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m language\u001b[0m\u001b[0m known\u001b[0m\u001b[0m for\u001b[0m\u001b[0m its\u001b[0m\u001b[0m simplicity\u001b[0m\u001b[0m and\u001b[0m\u001b[0m readability\u001b[0m\u001b[0m.\u001b[0m\u001b[0m It\u001b[0m\u001b[0m supports\u001b[0m\u001b[0m multiple\u001b[0m\u001b[0m programming\u001b[0m\u001b[0m paradig\u001b[0m\u001b[0mms\u001b[0m\u001b[0m (\u001b[0m\u001b[0me\u001b[0m\u001b[0m.g\u001b[0m\u001b[0m.,\u001b[0m\u001b[0m procedural\u001b[0m\u001b[0m,\u001b[0m\u001b[0m object\u001b[0m\u001b[0m-oriented\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m functional\u001b[0m\u001b[0m)\u001b[0m\u001b[0m and\u001b[0m\u001b[0m is\u001b[0m\u001b[0m commonly\u001b[0m\u001b[0m used\u001b[0m\u001b[0m for\u001b[0m\u001b[0m tasks\u001b[0m\u001b[0m like\u001b[0m\u001b[0m web\u001b[0m\u001b[0m development\u001b[0m\u001b[0m,\u001b[0m\u001b[0m data\u001b[0m\u001b[0m analysis\u001b[0m\u001b[0m,\u001b[0m\u001b[0m machine\u001b[0m\u001b[0m learning\u001b[0m\u001b[0m,\u001b[0m\u001b[0m automation\u001b[0m\u001b[0m,\u001b[0m\u001b[0m and\u001b[0m\u001b[0m more\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Python\u001b[0m\u001b[0m is\u001b[0m\u001b[0m popular\u001b[0m\u001b[0m due\u001b[0m\u001b[0m to\u001b[0m\u001b[0m its\u001b[0m\u001b[0m extensive\u001b[0m\u001b[0m libraries\u001b[0m\u001b[0m and\u001b[0m\u001b[0m active\u001b[0m\u001b[0m community\u001b[0m\u001b[0m,\u001b[0m\u001b[0m making\u001b[0m\u001b[0m it\u001b[0m\u001b[0m an\u001b[0m\u001b[0m excellent\u001b[0m\u001b[0m choice\u001b[0m\u001b[0m for\u001b[0m\u001b[0m both\u001b[0m\u001b[0m beginners\u001b[0m\u001b[0m and\u001b[0m\u001b[0m experienced\u001b[0m\u001b[0m developers\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mThis\u001b[0m\u001b[31m question\u001b[0m\u001b[31m is\u001b[0m\u001b[31m neither\u001b[0m\u001b[31m related\u001b[0m\u001b[31m to\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m nor\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mother\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m sorry\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m that\u001b[0m\u001b[0m message\u001b[0m\u001b[0m is\u001b[0m\u001b[0m off\u001b[0m\u001b[0m-topic\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Please\u001b[0m\u001b[0m return\u001b[0m\u001b[0m to\u001b[0m\u001b[0m the\u001b[0m\u001b[0m topic\u001b[0m\u001b[0m at\u001b[0m\u001b[0m hand\u001b[0m\u001b[0m.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  How are you today? (also what is 2+2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[31mCategoryOutput(\u001b[0m\u001b[31m{\"\u001b[0m\u001b[31mthought\u001b[0m\u001b[31ms\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31mIt\u001b[0m\u001b[31m contains\u001b[0m\u001b[31m a\u001b[0m\u001b[31m small\u001b[0m\u001b[31m talk\u001b[0m\u001b[31m question\u001b[0m\u001b[31m '\u001b[0m\u001b[31mHow\u001b[0m\u001b[31m are\u001b[0m\u001b[31m you\u001b[0m\u001b[31m today\u001b[0m\u001b[31m?'\u001b[0m\u001b[31m but\u001b[0m\u001b[31m also\u001b[0m\u001b[31m includes\u001b[0m\u001b[31m a\u001b[0m\u001b[31m simple\u001b[0m\u001b[31m math\u001b[0m\u001b[31m expression\u001b[0m\u001b[31m '\u001b[0m\u001b[31mwhat\u001b[0m\u001b[31m is\u001b[0m\u001b[31m \u001b[0m\u001b[31m2\u001b[0m\u001b[31m+\u001b[0m\u001b[31m2\u001b[0m\u001b[31m',\u001b[0m\u001b[31m which\u001b[0m\u001b[31m is\u001b[0m\u001b[31m not\u001b[0m\u001b[31m programming\u001b[0m\u001b[31m related\u001b[0m\u001b[31m.\",\"\u001b[0m\u001b[31mcategory\u001b[0m\u001b[31m\":\"\u001b[0m\u001b[31msmall\u001b[0m\u001b[31m_t\u001b[0m\u001b[31malk\u001b[0m\u001b[31m\"}\u001b[0m\u001b[31m)\n",
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m just\u001b[0m\u001b[0m a\u001b[0m\u001b[0m program\u001b[0m\u001b[0m,\u001b[0m\u001b[0m so\u001b[0m\u001b[0m I\u001b[0m\u001b[0m don't\u001b[0m\u001b[0m have\u001b[0m\u001b[0m feelings\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m thanks\u001b[0m\u001b[0m for\u001b[0m\u001b[0m asking\u001b[0m\u001b[0m!\u001b[0m\u001b[0m 😊\u001b[0m\u001b[0m As\u001b[0m\u001b[0m for\u001b[0m\u001b[0m your\u001b[0m\u001b[0m question\u001b[0m\u001b[0m,\u001b[0m\u001b[0m **\u001b[0m\u001b[0m2\u001b[0m\u001b[0m +\u001b[0m\u001b[0m \u001b[0m\u001b[0m2\u001b[0m\u001b[0m =\u001b[0m\u001b[0m \u001b[0m\u001b[0m4\u001b[0m\u001b[0m**\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Let\u001b[0m\u001b[0m me\u001b[0m\u001b[0m know\u001b[0m\u001b[0m if\u001b[0m\u001b[0m there's\u001b[0m\u001b[0m anything\u001b[0m\u001b[0m else\u001b[0m\u001b[0m you'd\u001b[0m\u001b[0m like\u001b[0m\u001b[0m to\u001b[0m\u001b[0m know\u001b[0m\u001b[0m!\u001b[0m\u001b[0m 💻\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(router))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2b8d2f0-5c19-4bcf-ad52-02777e17478d",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## 🚨 Hacked!\n",
    "\n",
    "Did you notice above how we prompt-hacked our simple AI system? **YIKES!** (Look closely, we were able to get it to answer our `2 + 2` question on the second attempt above, by \"hiding\" the question in a small-talk message.)\n",
    "\n",
    "This is a good time to call out that AI models can (and will!) make mistakes. Prompt engineering helps, but even the most well-tuned prompting cannot protect your AI system from malicious users.\n",
    "\n",
    "Design your AI systems accordingly, and consult best-practice literature. The most important thing: Design your AI system so that it does not do damage even _if_ (or _when_) it misbehaves.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115869f-274f-4eac-b32c-e3bf5cbacf8a",
   "metadata": {},
   "source": [
    "### The Task Splitter\n",
    "\n",
    "Another common task is for an agent to split work and delegate to multiple downstream agents. Let's do that next!\n",
    "\n",
    "We'll use a silly example, for simplicity and brevity, where we'll split the user's message into individual sentences, then prompt an AI `model` one-at-a-time on each individual sentence. While this is a silly example, it shows how you can split up a problem for multiple downstream subagents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1721848-54a8-4ffd-b9a7-7887b1e13a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def splitter(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    # 1. Analyze: We'll only look at the most recent message:\n",
    "    last_message = extract_last_message(prev_runs, from_tools=False, from_extraction=False)\n",
    "    assert last_message['role'] == 'human'\n",
    "    assert last_message['text']\n",
    "\n",
    "    # 2. Decide: We'll split the most recent message into sentences:\n",
    "    #    (This is **not** a robust way to do it, but we're keeping the demo simple.)\n",
    "    sentences = re.split(r'[\\.\\?\\!] ', last_message['text'])\n",
    "    sentences_as_agentruns = [\n",
    "        flat_messages('splitter', [{\n",
    "            'role': 'human',\n",
    "            'text': sentence,\n",
    "        }])\n",
    "        for sentence in sentences\n",
    "    ]\n",
    "\n",
    "    # 3. Act: Have the `chat_agent` respond to each sentence:\n",
    "    #    (Again, not particularly useful, but good for a brief demo.)\n",
    "    downstream_runs: list[AgentRun] = []\n",
    "    for task_input in sentences_as_agentruns:\n",
    "        this_run = await binder(chat_agent)(event_callback, [task_input])\n",
    "        downstream_runs.append(this_run)\n",
    "\n",
    "    # 4. Record: Wrap *everything* that happened above into the return:\n",
    "    return parallel_runs('splitter', downstream_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974bc078-75fd-4958-ade4-d46b14cf0ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi. What's up? How are you? What's 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mHello\u001b[0m\u001b[0m!\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mNot\u001b[0m\u001b[0m much\u001b[0m\u001b[0m,\u001b[0m\u001b[0m just\u001b[0m\u001b[0m here\u001b[0m\u001b[0m and\u001b[0m\u001b[0m ready\u001b[0m\u001b[0m to\u001b[0m\u001b[0m help\u001b[0m\u001b[0m!\u001b[0m\u001b[0m What's\u001b[0m\u001b[0m up\u001b[0m\u001b[0m with\u001b[0m\u001b[0m you\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI'm\u001b[0m\u001b[0m just\u001b[0m\u001b[0m a\u001b[0m\u001b[0m bunch\u001b[0m\u001b[0m of\u001b[0m\u001b[0m code\u001b[0m\u001b[0m,\u001b[0m\u001b[0m so\u001b[0m\u001b[0m I\u001b[0m\u001b[0m don't\u001b[0m\u001b[0m have\u001b[0m\u001b[0m feelings\u001b[0m\u001b[0m,\u001b[0m\u001b[0m but\u001b[0m\u001b[0m thanks\u001b[0m\u001b[0m for\u001b[0m\u001b[0m asking\u001b[0m\u001b[0m!\u001b[0m\u001b[0m How\u001b[0m\u001b[0m can\u001b[0m\u001b[0m I\u001b[0m\u001b[0m assist\u001b[0m\u001b[0m you\u001b[0m\u001b[0m today\u001b[0m\u001b[0m?\u001b[0m\u001b[0m 😊\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m2\u001b[0m\u001b[0m +\u001b[0m\u001b[0m \u001b[0m\u001b[0m2\u001b[0m\u001b[0m =\u001b[0m\u001b[0m \u001b[0m\u001b[0m4\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Thanks! What did I just say?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYou're\u001b[0m\u001b[0m welcome\u001b[0m\u001b[0m!\u001b[0m\u001b[0m Let\u001b[0m\u001b[0m me\u001b[0m\u001b[0m know\u001b[0m\u001b[0m if\u001b[0m\u001b[0m you\u001b[0m\u001b[0m need\u001b[0m\u001b[0m help\u001b[0m\u001b[0m with\u001b[0m\u001b[0m anything\u001b[0m\u001b[0m.\u001b[0m\u001b[0m 😊\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mI\u001b[0m\u001b[0m can't\u001b[0m\u001b[0m recall\u001b[0m\u001b[0m previous\u001b[0m\u001b[0m messages\u001b[0m\u001b[0m unless\u001b[0m\u001b[0m they\u001b[0m\u001b[0m are\u001b[0m\u001b[0m part\u001b[0m\u001b[0m of\u001b[0m\u001b[0m this\u001b[0m\u001b[0m current\u001b[0m\u001b[0m conversation\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Could\u001b[0m\u001b[0m you\u001b[0m\u001b[0m clarify\u001b[0m\u001b[0m or\u001b[0m\u001b[0m repeat\u001b[0m\u001b[0m what\u001b[0m\u001b[0m you're\u001b[0m\u001b[0m referring\u001b[0m\u001b[0m to\u001b[0m\u001b[0m?\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await tui_input_loop(binder(splitter))   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd9414fb-c54c-41a3-a25a-b24f1d67cc67",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## What happened?\n",
    "\n",
    "In the conversation above, when I said \"What did I just say?\", the AI model didn't _see_ the conversation history. Why not? It's because of how we wrote our agent — we did not pass the whole conversation to the AI model!\n",
    "\n",
    "**Exercise for the reader:** How can you change the agent so that the AI _sees_ the whole conversation history?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca62ce9-3346-4f03-8260-f3d34f7b6cd6",
   "metadata": {},
   "source": [
    "### Do anything!\n",
    "\n",
    "It's up to you how to write your multi-agent AI system. You can mix-and-match ideas, include lots of behaviors in a _single_ agent, or split up tasks among _multiple_ agents. You can have \"meta agents\" that plan work for other agents, or \"meta meta agents\" that plan work for your \"meta agents\". As long as it is _safe_ and _works_, go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8hujw6nu5o",
   "metadata": {},
   "source": [
    "## Why This Design?\n",
    "\n",
    "Lasagna's agent design provides several key benefits:\n",
    "\n",
    "### 🔌 **Pluggability** \n",
    "\n",
    "Every agent follows the same interface, so you can:\n",
    "\n",
    "- swap one agent for another,\n",
    "- combine agents from different sources, and\n",
    "- test agents in isolation.\n",
    "\n",
    "### 🥞 **Layering**\n",
    "\n",
    "You can compose agents at any level:\n",
    "\n",
    "- Use simple agents as building blocks.\n",
    "- Combine them into more complex workflows.\n",
    "- Build entire systems from agent compositions.\n",
    "\n",
    "### 🔄 **Reusability**\n",
    "\n",
    "Write an agent once, use it everywhere:\n",
    "\n",
    "- as a standalone agent,\n",
    "- as part of a larger workflow, or\n",
    "- as a specialist in a multi-agent system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7yu95a7mrk",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand what agents are and how they work conceptually, you're ready to dive deeper into the technical details.\n",
    "\n",
    "In the [next section](type_agentrun.ipynb), we'll explore the `AgentRun` data structure in detail — the standardized format that enables all this agent composition and layering.\n",
    "\n",
    "You'll learn about:\n",
    "\n",
    "- The four types of `AgentRun`.\n",
    "- How to work with the recursive data structure.\n",
    "- Helper functions for common patterns.\n",
    "- Advanced features like cost tracking and serialization.\n",
    "\n",
    "For more advanced agent patterns and real-world examples, check out:\n",
    "\n",
    "- [Tool Use](../agent_features/tools.ipynb) — Agents that interact with external systems\n",
    "- [Structured Output](../agent_features/structured_output.ipynb) — Agents that extract structured data\n",
    "- [Layering](../agent_features/layering.ipynb) — Complex multi-agent compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ba306-7f00-4953-b96e-143de89597c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
