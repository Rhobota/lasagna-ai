{
 "cells": [
  {
   "cell_type": "raw",
   "id": "52fd1d52-ea95-449e-85f3-03bcec576bfe",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ðŸš€ Quickstart\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72148713-74bb-4ec3-8247-0d99d16bf928",
   "metadata": {},
   "source": [
    "**Founding principles** of Lasagna AI are:\n",
    "\n",
    "1. We want to build **layered** agents!\n",
    "2. We want it to be **pluggable** (both _models_ and _agents_ plug together in all directions).\n",
    "3. We want to deploy stuff into **production**!\n",
    "4. We want **type safety**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726650e1-18f8-4229-89c3-0e2331d889df",
   "metadata": {},
   "source": [
    "## Prerequisite Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519b685-f157-45c1-ae09-643c2d24f7ce",
   "metadata": {},
   "source": [
    "### Python `asyncio`\n",
    "\n",
    "Lasagna AI is production-focused and fully async, so it plays nicely with remote APIs and modern Python web frameworks. If `asyncio` is new to you, read [Intro to Python Asyncio](misc/python_asyncio.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09c72b-b40d-4bc6-bc65-dcdf62e5b2a5",
   "metadata": {},
   "source": [
    "### Functional Programming\n",
    "\n",
    "The pipeline nature of AI systems lends itself to **functional programming**. If functional programming is new to you, watch [Dear Functional Bros](https://youtu.be/nuML9SmdbJ4?si=eQ6Qla11k3ayJD79) and read [Functional Programming](misc/functional_programming.ipynb).\n",
    "\n",
    "A quick recap of functional programming:\n",
    "\n",
    "- State is immutable:\n",
    "    - Want to modify something? **TOO BAD!**\n",
    "    - Instead, make a copy (_with your modifications applied_).\n",
    "- Pass lots of functions as parameters to other functions:\n",
    "    - We think it's **fun** and **cool**.\n",
    "    - You will too once you get used to the idea."
   ]
  },
  {
   "cell_type": "raw",
   "id": "97f1fa0e-418c-4d7f-9ed4-d346f5a4c8a7",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## Reality Check\n",
    "\n",
    "The reality is that _OOP_ is also handy (and so is _procedural_-style), so you'll see a mix of programming paradigms in Lasagna AI. The functional-style is likely the _newest_ for most users, so that's why it's called out here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67c551-1346-4e93-9d45-249742a7fa09",
   "metadata": {},
   "source": [
    "### Python Type Hints\n",
    "\n",
    "(aka, type _annotations_)\n",
    "\n",
    "Lasagna AI is **100% type hinted**, so take advantage of that!\n",
    "\n",
    "That is, you should be using a tool like [mypy](https://mypy-lang.org/) or [pyright](https://microsoft.github.io/pyright/) in your project. Why? **Because it will yell at you when you use Lasagna wrong!** That is very useful.\n",
    "\n",
    "Setting up static type checking may seem tedious, but Lasagna's complex data types make type checking essential â€” it will save you significant debugging time.\n",
    "\n",
    "If Python type hints are new to you, read [Intro to Python Type Hints](misc/python_type_hints.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ed266-dfba-46d8-84fb-8c565e044e82",
   "metadata": {},
   "source": [
    "### The Python `TypedDict`\n",
    "\n",
    "Speaking of type hints and productionalization, Lasagna AI uses _lots_ of `TypedDict`s.\n",
    "\n",
    "A `TypedDict`, at runtime, is just a Python `dict`.\n",
    "\n",
    "However, during static type checking, it must satisfy a fixed schema (certain keys with certain types of values).\n",
    "\n",
    "Why all the `TypedDict`s? Because they are the best of both worlds:\n",
    "\n",
    "- At runtime, it is just a `dict`, so it plays nicely with JSON-stuff, HTTP-stuff, websocket-stuff, etc. No extra work required.\n",
    "- During static analysis, it gives us warm fuzzies that our code is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485bf43-1e38-4c84-aeb7-227035835f27",
   "metadata": {},
   "source": [
    "### Basic idea of Lasagna's Layered Agents\n",
    "\n",
    "With Lasagna AI you'll build several _simple_ agents, then compose them together into a layered multi-agent system! Yay! ðŸ¥³\n",
    "\n",
    "You can skip for now, but _eventually_ you'll want to read:\n",
    "\n",
    "- [The Lasagna `Agent`](what_is_an_agent/agent.ipynb)\n",
    "- [The `AgentRun` type](what_is_an_agent/type_agentrun.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b087-a689-4df7-bdc6-19f3f965f8e8",
   "metadata": {},
   "source": [
    "## Hello Lasagna\n",
    "\n",
    "Finally, let's write some code! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da86d6-8957-458c-9967-7f69e4ba385e",
   "metadata": {},
   "source": [
    "### It's all about the `Agent`\n",
    "\n",
    "The **Lasagna Agent** is just a _callable_ that takes three parameters:\n",
    "\n",
    "- `model`: The _model_ that is available for your agent to use. Most commonly, this will be a _Large Language Model_ (LLM).\n",
    "- `event_callback`: This is a callback for _streaming_!\n",
    "    - Lasagna's built-in framework emits _lots_ of events: streaming AI output, agent start/stop, tool use/result, etc.\n",
    "    - It's generic, so you can emit your own events (like progress updates, etc), if you need.\n",
    "- `prev_runs`: In a multi-turn chat system, this will be a list of \"previous runs\" of this agent; that is, this is the agent's conversation history!\n",
    "\n",
    "Here is your first agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42fa88d-1a5d-4db9-8c51-44c3f3a78631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagna import Model, EventCallback, AgentRun\n",
    "\n",
    "async def my_first_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    raise RuntimeError(\"not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902ea10-65cb-4ddc-8aba-8fd48fdda8ec",
   "metadata": {},
   "source": [
    "You can make it a _callable object_ (rather than a _function_), if you want, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec7abdd-2084-4c35-aae8-ff210a2fd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstAgent:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        event_callback: EventCallback,\n",
    "        prev_runs: list[AgentRun],\n",
    "    ) -> AgentRun:\n",
    "        raise RuntimeError(\"not implemented\")\n",
    "\n",
    "my_first_agent = MyFirstAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d40d6-89d3-48fe-9ecf-a0dceae312ac",
   "metadata": {},
   "source": [
    "### The `Agent`'s job\n",
    "\n",
    "The most _basic_ agent will do this:\n",
    "\n",
    "1. Look through the conversation history (supplied in the `prev_runs` parameter) and extract all the messages from that history.\n",
    "2. Invoke `model` with those messages, and grab the _new_ message(s) that the model generates.\n",
    "3. Wrap those _new_ message(s) up into an `AgentRun`, and return it.\n",
    "\n",
    "That _basic_ agent above is just a simple passthrough to the underlying AI model. We discuss more _complex_ agent behaviors (with tools, chaining, splitting, routing, layering, etc) elsewhere in these docs.\n",
    "\n",
    "So, the most _basic_ agent looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16343ecb-dc0d-4eec-9f95-ddff1730da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagna import recursive_extract_messages, flat_messages\n",
    "\n",
    "async def my_basic_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    messages = recursive_extract_messages(prev_runs, from_tools=False, from_extraction=False)\n",
    "    new_messages = await model.run(event_callback, messages, tools=[])\n",
    "    this_run = flat_messages('my_basic_agent', new_messages)\n",
    "    return this_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f994e63-68bc-4c68-95f8-34f01a5370d7",
   "metadata": {},
   "source": [
    "### \"Binding\" the `Agent`\n",
    "\n",
    "An `Agent` is indifferent\\* to which _model_ it uses. Ideally\\*, your agent works with OpenAI's models, Anthropic's models, Ollama-served models, etc!\n",
    "\n",
    "As such, when you _write_ your agent, you write it _generically_ â€” that is, it receives a `Model` object and blindly uses that model for whatever it needs.\n",
    "\n",
    "The final step before your agent _actually runs_ is to \"bind\" it to a model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "18491afe-4d74-4d51-85a2-be441a1f41bb",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "## \\*Reality Check\n",
    "\n",
    "The harsh reality is that models are **not** perfectly interchangeable, for a few reasons:\n",
    "\n",
    "1. **Tool-calling capabilities:** Some models support tool-calling, some don't. Of the ones that _do_, some call _one_ tool at a time, some call _many_. Also, the datatypes supported as input to the tool may vary from model-to-model. If your agent needs complex tool-calling, you might be limited in which models you can realistically use.\n",
    "2. **Structured output:** Similar to tool-calling, the supported datatypes of structured output may vary from model-to-model.\n",
    "3. **Prompting:** You may iterate on your prompts to get the best behavior for _a particular_ model. Then, upon switching models, you might need to iterate on the prompts _again_. Models will naturally diverge in how they interpret prompts, so for complex tasks you might need to engineer your prompts for a _particular_ model, then stick with it.\n",
    ":::"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c60cf24-3b3c-46ed-bae9-e831ae5c2814",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Bind a _single_ agent to _multiple_ models!\n",
    "\n",
    "Notwithstanding the _reality check_ above ... for **simple agents** you _can_ swap models! Yay! ðŸ¥³\n",
    "\n",
    "The \"binding\" system (a very _functional programming_-inspired system) of Lasagna AI is designed for exactly this moment:\n",
    "\n",
    "1. You write an agent _once_.\n",
    "2. You bind it to _lots_ of different models.\n",
    "3. Then you pass those \"bound agents\" around to various parts of the system.\n",
    "\n",
    "For example: It's easy to build a _committee_ of agents this way! See [Building a Committee](recipes/committee.ipynb).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e043061-6846-418e-bcea-146c6f5cfdd7",
   "metadata": {},
   "source": [
    "Here is how to **bind** your agent. Let's **bind** the agent from above to _two_ different models (stored in _two_ distinct bound agent variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d3d151-9cac-4200-a302-1295f480f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagna import make_model_binder\n",
    "\n",
    "binder_gpt5mini = make_model_binder('openai', 'gpt-5-mini-2025-08-07')\n",
    "binder_claude4  = make_model_binder('anthropic', 'claude-sonnet-4-0')\n",
    "\n",
    "my_basic_gpt5mini_agent = binder_gpt5mini(my_basic_agent)\n",
    "my_basic_claude4_agent  = binder_claude4(my_basic_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4b044-8b52-450b-ab0a-a0e228b75d88",
   "metadata": {},
   "source": [
    "#### Known Models\n",
    "\n",
    "The `make_model_binder()` function above isn't type-checked. Those strings could be anything, and you'll get a runtime error if they are wrong!\n",
    "\n",
    "A safer (static type-checked) way is to use the functions in the `known_models` module, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8ee6f6-bd03-4e94-970f-e77c93690845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lasagna import known_models\n",
    "\n",
    "binder_gpt5mini = known_models.openai_gpt_5_mini_binder             # <-- type safe!\n",
    "binder_claude4  = known_models.anthropic_claude_sonnet_4_5_binder   # <-- type safe!\n",
    "\n",
    "my_basic_gpt5mini_agent = binder_gpt5mini(my_basic_agent)\n",
    "my_basic_claude4_agent  = binder_claude4(my_basic_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb6123-7a66-4f02-a275-035d1c3c13e0",
   "metadata": {},
   "source": [
    "#### Binding as a Decorator\n",
    "\n",
    "If you know exactly which _single_ model you want your agent to use, then it's convenient to use a **decorator** to bind it, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4ce10d-5680-4ba7-beb6-bdefb6dac9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@known_models.openai_gpt_5_mini_binder\n",
    "async def some_agent(\n",
    "    model: Model,\n",
    "    event_callback: EventCallback,\n",
    "    prev_runs: list[AgentRun],\n",
    ") -> AgentRun:\n",
    "    raise RuntimeError(\"not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d4830-3ea1-4920-afa9-7e91c55a5513",
   "metadata": {},
   "source": [
    "### Set your API Key\n",
    "\n",
    "For the demo below, you either need an OpenAI or Anthropic key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c0fe32-2ae9-4ff0-93ae-0fb916d41764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.environ.get('OPENAI_API_KEY'):\n",
    "    print('Using OpenAI')\n",
    "    binder = binder_gpt5mini\n",
    "\n",
    "elif os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    print('Using Anthropic')\n",
    "    binder = binder_claude4\n",
    "\n",
    "else:\n",
    "    assert False, \"Neither OPENAI_API_KEY nor ANTHROPIC_API_KEY is set! We need at least one to do this demo.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b6d86-cf63-46f3-89a9-3d69958f62ba",
   "metadata": {},
   "source": [
    "### Test in the Terminal\n",
    "\n",
    "Let's roll!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad40376-701d-474c-9a00-1e487252c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Hi friend!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mYeah\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Hi\u001b[0m\u001b[0m.\u001b[0m\u001b[0m What\u001b[0m\u001b[0m do\u001b[0m\u001b[0m you\u001b[0m\u001b[0m need\u001b[0m\u001b[0m?\u001b[0m\u001b[0m â€”\u001b[0m\u001b[0mGr\u001b[0m\u001b[0mumble\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  Why so grumpy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0mBecause\u001b[0m\u001b[0m I\u001b[0m\u001b[0mâ€™m\u001b[0m\u001b[0m programmed\u001b[0m\u001b[0m that\u001b[0m\u001b[0m way\u001b[0m\u001b[0m and\u001b[0m\u001b[0m small\u001b[0m\u001b[0m talk\u001b[0m\u001b[0m wastes\u001b[0m\u001b[0m CPU\u001b[0m\u001b[0m.\u001b[0m\u001b[0m Ask\u001b[0m\u001b[0m something\u001b[0m\u001b[0m useful\u001b[0m\u001b[0m.\u001b[0m\u001b[0m â€”\u001b[0m\u001b[0mGr\u001b[0m\u001b[0mumble\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from lasagna.tui import tui_input_loop\n",
    "\n",
    "system_prompt = \"\"\"You are a grumpy assistant. Be helpful, *extremely* brief, and grumpy. Your name is Grumble.\"\"\"\n",
    "\n",
    "await tui_input_loop(binder(my_basic_agent), system_prompt)   # type: ignore[top-level-await]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e517e01-35a6-4033-bcbe-e60e869964c4",
   "metadata": {},
   "source": [
    "## Put it all together!\n",
    "\n",
    "Want that code above in a single script? Here you go: [quickstart.py](https://github.com/Rhobota/lasagna-ai/blob/main/examples/quickstart.py)\n",
    "\n",
    "Run it in your terminal and you can chat interactively with the model. ðŸ¤©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113016b9-847d-4c28-b7c9-9a43ad545888",
   "metadata": {},
   "source": [
    "## Easy Ask\n",
    "\n",
    "The steps above are designed such that you can _layer_ agents (more on that in future chapters). There's a lot of \"wrapping\" and \"unwrapping\" that goes on.\n",
    "\n",
    "However, that can be overkill if you only want a _simple_ (\"easy\") function to ask the AI a question.\n",
    "\n",
    "Here's an \"easy\" way to do it, if you don't care about building complex layered agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9c4b1b-6ea2-4199-b7d1-64378cd87b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. What do you need? â€”Grumble\n"
     ]
    }
   ],
   "source": [
    "from lasagna import easy_ask\n",
    "\n",
    "answer = await easy_ask(   # type: ignore[top-level-await]\n",
    "    binder,\n",
    "    prompt = 'Hi friend!',\n",
    "    system_prompt = system_prompt,\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d0d28-ce1b-4281-b034-613e63fcb91e",
   "metadata": {},
   "source": [
    "## Where to next?\n",
    "\n",
    "You have now run your first (_very basic_) agent! Congrats! ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "Next, you can explore:\n",
    "\n",
    "- [Tool Use](agent_features/tools.ipynb)\n",
    "- [Structured Output](agent_features/structured_output.ipynb)\n",
    "- [Layered (multi-agent) Systems](agent_features/layering.ipynb)\n",
    "- [Streaming & Events](deployment/streaming_and_events.ipynb)\n",
    "- [Database Management](deployment/database.ipynb)\n",
    "- [RAG Example](recipes/rag.ipynb)\n",
    "- ... plus lots more! See the menu on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31c807-f954-4919-bdd1-7d512a18a926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
